{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About this repository This repo is the canonical source for Kubernetes Operators that appear on OperatorHub.io , OpenShift Container Platform and OKD . Add your Operator We would love to see your Operator being added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start here . If you have an existing Operator read our contribution guidelines on how to package and test it. Then test your Operator locally and submit a Pull Request. Test your Operator before submitting a PR You can use our test suite to test your Operator prior to submitting it. Our test suite will help you to install it. Then assuming you followed the contribution guide, you can run the entire suite on a Linux or macOS system with Docker installed: cd <community-operators-project> bash < ( curl -sL https://cutt.ly/WhkV76k ) \\ kiwi,lemon,orange \\ <operator-stream>/<operator-name>/<operator-version> Tests are not passing or you want to know more? Check test suite for more info. Preview your Operator on OperatorHub.io You can preview how your Operator would be rendered there by using the preview tool . If you are submitting your Operator in the upstream-community-operators directory your Operator will appear on OperatorHub.io. Submitting your PR Review this checklist upon creating a PR and after you acknowledged the contribution guidelines. Do not forget to add ci.yaml to the top level of your operator. Otherwise only semver mode will be supported. Update your Operator Similarly, to update your operator you need to submit a PR with any changes to your Operator resources. Refer to our contribution guide for more details. CI Tests your Operator Upon creating a pull request against this repo, a set of CI pipelines will run, see more details here . The pipeline will actually run the same commands you use to test locally. You can help speed up the review of your PR by testing locally, either manually or using scripts . For troubleshooting failing tests consult the manual test steps or see specific error messages solved in troubleshooting guide . Reporting Bugs Use the issue tracker in this repository to report bugs.","title":"Overview"},{"location":"#about-this-repository","text":"This repo is the canonical source for Kubernetes Operators that appear on OperatorHub.io , OpenShift Container Platform and OKD .","title":"About this repository"},{"location":"#add-your-operator","text":"We would love to see your Operator being added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start here . If you have an existing Operator read our contribution guidelines on how to package and test it. Then test your Operator locally and submit a Pull Request.","title":"Add your Operator"},{"location":"#test-your-operator-before-submitting-a-pr","text":"You can use our test suite to test your Operator prior to submitting it. Our test suite will help you to install it. Then assuming you followed the contribution guide, you can run the entire suite on a Linux or macOS system with Docker installed: cd <community-operators-project> bash < ( curl -sL https://cutt.ly/WhkV76k ) \\ kiwi,lemon,orange \\ <operator-stream>/<operator-name>/<operator-version> Tests are not passing or you want to know more? Check test suite for more info.","title":"Test your Operator before submitting a PR"},{"location":"#preview-your-operator-on-operatorhubio","text":"You can preview how your Operator would be rendered there by using the preview tool . If you are submitting your Operator in the upstream-community-operators directory your Operator will appear on OperatorHub.io.","title":"Preview your Operator on OperatorHub.io"},{"location":"#submitting-your-pr","text":"Review this checklist upon creating a PR and after you acknowledged the contribution guidelines. Do not forget to add ci.yaml to the top level of your operator. Otherwise only semver mode will be supported.","title":"Submitting your PR"},{"location":"#update-your-operator","text":"Similarly, to update your operator you need to submit a PR with any changes to your Operator resources. Refer to our contribution guide for more details.","title":"Update your Operator"},{"location":"#ci-tests-your-operator","text":"Upon creating a pull request against this repo, a set of CI pipelines will run, see more details here . The pipeline will actually run the same commands you use to test locally. You can help speed up the review of your PR by testing locally, either manually or using scripts . For troubleshooting failing tests consult the manual test steps or see specific error messages solved in troubleshooting guide .","title":"CI Tests your Operator"},{"location":"#reporting-bugs","text":"Use the issue tracker in this repository to report bugs.","title":"Reporting Bugs"},{"location":"action/","text":"Community operator action V1 This action runs community operator tests. What's new Supported tests (kiwi,lemon,orange) Own community-operators fork and branch supported Run test from own repository. Doesn't have to be community-operators . More info in op-action-examples Usage - uses : operator-framework/community-operators@v1 with : # Test type (kiwi,lemon or orange) test-type : 'kiwi' # Operator stream name (community-operators or upstream-community-operators) stream : 'community-operators' # Operator name (exmaple 'aqua') name : '' # Operator version (exmaple '5.3.0') version : '' # Community operators repo # Default: 'https://github.com/operator-framework/community-operators.git' repo : '' # Community operators branch # Default: 'master' branch : '' # Repo directory when if not community-operators # Default: 'community-operators' repo-dir : '' # Space separated list of labels in PR # Default: '' pr-labels : '' # Path to operator version content (for example local/path/to/operator/version). # Default: '' operator-version-path : '' # Path to package file (for example local/path/to/my-operator.package.yaml). # Default: '' package-path : '' # Path to ci.yaml file (for example local/path/to/ci.yaml). # Default: '' ci-path : '' Test 'kiwi' aqua operator version 5.3.0 for in community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'kiwi' aqua operator version 5.3.0 for in upstream-community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'lemon' aqua operator version 5.3.0 for in upstream-community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'lemon' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' Test 'orange' (catalog v4.6) for aqua operator version 5.3.0 for in community-operators - uses : operator-framework/community-operators@v1 with : test-type : 'oragne_v4.6' stream : 'community-operators' name : 'aqua' version : '5.3.0' Test 'kiwi' aqua operator version 5.3.0 for in upstream-community-operators in own project Test single version of operator from custom project. Follwoing will happen: Action will clone https://github.com/operator-framework/community-operators.git in to master branch (controlled by repo: and branch: ) Enters directory community-operators (controlled by repo-dir: ) Removes directory upstream-community-operators/aqua/5.3.0 Creates directory upstream-community-operators/aqua/5.3.0 Copy content my/op/manifest to upstream-community-operators/aqua/5.3.0 (controlled by operator-version-path: ) Copy/overwrite my/op/aqua-operator.package.yaml to upstream-community-operators/aqua/ (controlled by package-path: ) Copy/overwrite my/op/ci.yaml to upstream-community-operators/aqua/ (controlled by ci-path: ) Runs kiwi test (controlled by test-type: ) - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' repo : 'https://github.com/operator-framework/community-operators.git' branch : 'master' operator-version-path : my/op/manifest package-path : my/op/aqua-operator.package.yaml ci-path : my/op/ci.yaml License The scripts and documentation in this project are released under the MIT License","title":"Github Action"},{"location":"action/#community-operator-action-v1","text":"This action runs community operator tests.","title":"Community operator action V1"},{"location":"action/#whats-new","text":"Supported tests (kiwi,lemon,orange) Own community-operators fork and branch supported Run test from own repository. Doesn't have to be community-operators . More info in op-action-examples","title":"What's new"},{"location":"action/#usage","text":"- uses : operator-framework/community-operators@v1 with : # Test type (kiwi,lemon or orange) test-type : 'kiwi' # Operator stream name (community-operators or upstream-community-operators) stream : 'community-operators' # Operator name (exmaple 'aqua') name : '' # Operator version (exmaple '5.3.0') version : '' # Community operators repo # Default: 'https://github.com/operator-framework/community-operators.git' repo : '' # Community operators branch # Default: 'master' branch : '' # Repo directory when if not community-operators # Default: 'community-operators' repo-dir : '' # Space separated list of labels in PR # Default: '' pr-labels : '' # Path to operator version content (for example local/path/to/operator/version). # Default: '' operator-version-path : '' # Path to package file (for example local/path/to/my-operator.package.yaml). # Default: '' package-path : '' # Path to ci.yaml file (for example local/path/to/ci.yaml). # Default: '' ci-path : ''","title":"Usage"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-in-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'kiwi' aqua operator version 5.3.0 for in community-operators"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-in-upstream-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'kiwi' aqua operator version 5.3.0 for in upstream-community-operators"},{"location":"action/#test-lemon-aqua-operator-version-530-for-in-upstream-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'lemon' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'lemon' aqua operator version 5.3.0 for in upstream-community-operators"},{"location":"action/#test-orange-catalog-v46-for-aqua-operator-version-530-for-in-community-operators","text":"- uses : operator-framework/community-operators@v1 with : test-type : 'oragne_v4.6' stream : 'community-operators' name : 'aqua' version : '5.3.0'","title":"Test 'orange' (catalog v4.6) for aqua operator version 5.3.0 for in community-operators"},{"location":"action/#test-kiwi-aqua-operator-version-530-for-in-upstream-community-operators-in-own-project","text":"Test single version of operator from custom project. Follwoing will happen: Action will clone https://github.com/operator-framework/community-operators.git in to master branch (controlled by repo: and branch: ) Enters directory community-operators (controlled by repo-dir: ) Removes directory upstream-community-operators/aqua/5.3.0 Creates directory upstream-community-operators/aqua/5.3.0 Copy content my/op/manifest to upstream-community-operators/aqua/5.3.0 (controlled by operator-version-path: ) Copy/overwrite my/op/aqua-operator.package.yaml to upstream-community-operators/aqua/ (controlled by package-path: ) Copy/overwrite my/op/ci.yaml to upstream-community-operators/aqua/ (controlled by ci-path: ) Runs kiwi test (controlled by test-type: ) - uses : operator-framework/community-operators@v1 with : test-type : 'kiwi' stream : 'upstream-community-operators' name : 'aqua' version : '5.3.0' repo : 'https://github.com/operator-framework/community-operators.git' branch : 'master' operator-version-path : my/op/manifest package-path : my/op/aqua-operator.package.yaml ci-path : my/op/ci.yaml","title":"Test 'kiwi' aqua operator version 5.3.0 for in upstream-community-operators in own project"},{"location":"action/#license","text":"The scripts and documentation in this project are released under the MIT License","title":"License"},{"location":"best-practices/","text":"Operator Best Practices Development Considerations for operator developers An Operator should manage a single type of application, essentially following the UNIX principle: do one thing and do it well. If an application consists of multiple different tiers or components, multiple Operators should be written for each of them. If the application for example consists of Redis, AMQ and MySQL, there should be 3 Operators, not one. If there is significant orchestration and sequencing involved, an Operator should be written that represents the entire stack, in turn delegating to other Operators for orchestrating their part of it. Operators should own a CRD and only one Operator should control a CRD on cluster. Two Operators managing the same CRD calls for trouble. In case where an API exists but with multiple implementations, we typically have a no-op Operator (without any deployment/reconciliation loop) that defines that shared API and other Operators depend on this Operator to provide one implementation of the API, e.g. similar to PVCs or Ingress. Inside an Operator, multiple controllers should be used if multiple CRDs are managed. This helps with separation of concerns and code readability. Note that this doesn't necessarily mean one container image per controller, but rather one reconciliation loop (which could be running as part of the same operator binary) per CRD. An operator shouldn't deploy or manage other operators (such patterns are known as meta or super operators). It's the Operator Lifecycle Manager's job to manage the deployment and lifecycle of operators. If multiple operators must be packaged/shipped as a single entity (via the same CSV), then add all owned and required CRDs, as well as all deployments for operators that manage the owned CRDs, to the same CSV. Writing an Operator involves using the Kubernetes API and there is always the same boilerplate code required to connect and interact with it. Use a framework like Operator-SDK to save yourself time with this and get a suite of tooling to ease development and testing. Operators shouldn\u2019t make any assumptions about the namespace they are deployed in or hard-code names of resources that they expect to already exist. Operators shouldn\u2019t hard code the namespaces they are watching. This should be configurable - having no namespace supplied is interpreted as watching all namespaces Semantic versioning (aka semver) should be used to version an Operator. Operators are long-running workloads on the cluster and it\u2019s APIs are potentially in need of support over a longer period of time. Use the semver.org guidelines to help determine when and how to bump versions when there are breaking or non-breaking changes. Kubernetes API versioning guidelines should be used to version Operator CRDs. Use the Kubernetes sig-architecture guidelines to get best practices on when to bump versions and when breaking changes are acceptable. When defining CRDs, you should use OpenAPI spec to create a structural schema for your CRDs. Operators are instrumented to provide useful, actionable metrics to external systems (e.g. monitoring/alerting platforms). Minimally, metrics should represent the software's health and key performance indicators, as well as support the creation of service levels indicators such as throughput, latency, availability, errors, capacity, etc. Summary One Operator per managed application Multiple operators should be used for complex, multi-tier application stacks CRD can only be owned by a single Operator, shared CRDs should be owned by a separate Operator One controller per custom resource definition Use a framework like Operator SDK Do not hard-code namespaces or resources names Make watch namespace configurable Use semver / observe Kubernetes guidelines on versioning APIs Use OpenAPI spec with structural schema on CRDs Operators expose metrics to external systems Running On-Cluster Considerations for on-cluster behavior Like all containers on Kubernetes, Operators shouldn\u2019t need to run as root unless absolutely necessary. Operators should come with their own ServiceAccount and not rely on the default . Operators should not self-register their CRDs. These are global resources and careful consideration needs to be taken when setting those up. Also this requires the Operator to have global privileges which is potentially dangerous compared to that little extra convenience. Operators use CRs as the primary interface to the cluster user. As such, at all times, meaningful status information should be written to those objects unless they are solely used to store data in a structured schema. Operators should update according to semver and should be updated frequently. Operators need to support updating managed applications (Operands) that were set up by an older version of the Operator. There are multiple models for this: operator fan-out - where the operator allows the user to specify the version in the custom resource. single version - where the operator is tied to the version of the operand. hybrid approach - where the operator is tied to a range of versions, and the user can select some level of the version. An Operator should not deploy another Operator - an additional component on cluster should take care of this (OLM). When Operators change their APIs, CRD conversion (webhooks) should be used to deal with potentially older instances of them using the previous API version. Operators should make it easy for users to use their APIs - validating and rejecting malformed requests via extensive Open API validation schema on CRDs or via an admission webhook is good practice. The Operator itself should be really modest in its requirements - it should always be able to deploy by deploying its controllers, no user input should be required to start up the Operator. If user input is required to change the configuration of the Operator itself, a Configuration CRD should be used. Init-containers as part of the Operator deployments can be used to create a default instance of those CRs and then the Operator manages their lifecycle. Summary: An Operator... Does not run as root Does not self-register CRDs Does not install other Operators - rely on dependencies via package manager (OLM) Writes meaningful status information on Custom Resources objects unless pure data structure Should be capable of updating from a previous version of the Operator Should be capable of managing an Operand from an older Operator version Uses CRD conversion (webhooks) if API/CRDs change Uses OpenAPI validation / Admission Webhooks to reject invalid CRs Should always be able to deploy and come up without user input Offers (pre)configuration via a \u201cConfiguration CR\u201d instantiated by InitContainers","title":"Best practices"},{"location":"best-practices/#operator-best-practices","text":"","title":"Operator Best Practices"},{"location":"best-practices/#development","text":"Considerations for operator developers An Operator should manage a single type of application, essentially following the UNIX principle: do one thing and do it well. If an application consists of multiple different tiers or components, multiple Operators should be written for each of them. If the application for example consists of Redis, AMQ and MySQL, there should be 3 Operators, not one. If there is significant orchestration and sequencing involved, an Operator should be written that represents the entire stack, in turn delegating to other Operators for orchestrating their part of it. Operators should own a CRD and only one Operator should control a CRD on cluster. Two Operators managing the same CRD calls for trouble. In case where an API exists but with multiple implementations, we typically have a no-op Operator (without any deployment/reconciliation loop) that defines that shared API and other Operators depend on this Operator to provide one implementation of the API, e.g. similar to PVCs or Ingress. Inside an Operator, multiple controllers should be used if multiple CRDs are managed. This helps with separation of concerns and code readability. Note that this doesn't necessarily mean one container image per controller, but rather one reconciliation loop (which could be running as part of the same operator binary) per CRD. An operator shouldn't deploy or manage other operators (such patterns are known as meta or super operators). It's the Operator Lifecycle Manager's job to manage the deployment and lifecycle of operators. If multiple operators must be packaged/shipped as a single entity (via the same CSV), then add all owned and required CRDs, as well as all deployments for operators that manage the owned CRDs, to the same CSV. Writing an Operator involves using the Kubernetes API and there is always the same boilerplate code required to connect and interact with it. Use a framework like Operator-SDK to save yourself time with this and get a suite of tooling to ease development and testing. Operators shouldn\u2019t make any assumptions about the namespace they are deployed in or hard-code names of resources that they expect to already exist. Operators shouldn\u2019t hard code the namespaces they are watching. This should be configurable - having no namespace supplied is interpreted as watching all namespaces Semantic versioning (aka semver) should be used to version an Operator. Operators are long-running workloads on the cluster and it\u2019s APIs are potentially in need of support over a longer period of time. Use the semver.org guidelines to help determine when and how to bump versions when there are breaking or non-breaking changes. Kubernetes API versioning guidelines should be used to version Operator CRDs. Use the Kubernetes sig-architecture guidelines to get best practices on when to bump versions and when breaking changes are acceptable. When defining CRDs, you should use OpenAPI spec to create a structural schema for your CRDs. Operators are instrumented to provide useful, actionable metrics to external systems (e.g. monitoring/alerting platforms). Minimally, metrics should represent the software's health and key performance indicators, as well as support the creation of service levels indicators such as throughput, latency, availability, errors, capacity, etc.","title":"Development"},{"location":"best-practices/#running-on-cluster","text":"Considerations for on-cluster behavior Like all containers on Kubernetes, Operators shouldn\u2019t need to run as root unless absolutely necessary. Operators should come with their own ServiceAccount and not rely on the default . Operators should not self-register their CRDs. These are global resources and careful consideration needs to be taken when setting those up. Also this requires the Operator to have global privileges which is potentially dangerous compared to that little extra convenience. Operators use CRs as the primary interface to the cluster user. As such, at all times, meaningful status information should be written to those objects unless they are solely used to store data in a structured schema. Operators should update according to semver and should be updated frequently. Operators need to support updating managed applications (Operands) that were set up by an older version of the Operator. There are multiple models for this: operator fan-out - where the operator allows the user to specify the version in the custom resource. single version - where the operator is tied to the version of the operand. hybrid approach - where the operator is tied to a range of versions, and the user can select some level of the version. An Operator should not deploy another Operator - an additional component on cluster should take care of this (OLM). When Operators change their APIs, CRD conversion (webhooks) should be used to deal with potentially older instances of them using the previous API version. Operators should make it easy for users to use their APIs - validating and rejecting malformed requests via extensive Open API validation schema on CRDs or via an admission webhook is good practice. The Operator itself should be really modest in its requirements - it should always be able to deploy by deploying its controllers, no user input should be required to start up the Operator. If user input is required to change the configuration of the Operator itself, a Configuration CRD should be used. Init-containers as part of the Operator deployments can be used to create a default instance of those CRs and then the Operator manages their lifecycle.","title":"Running On-Cluster"},{"location":"contributing-prerequisites/","text":"Before submitting your Operator First off, thanks for taking the time to contribute your Operator! A primer to Kubernetes Community Operators This project collects Operators and publishes them to OperatorHub.io, a central registry for community-powered Kubernetes Operators. For OperatorHub.io your Operator needs to work with vanilla Kubernetes. This project also collects Community Operators that work with OpenShift to be displayed in the embedded OperatorHub. If you are new to Operators, start here . Sign Your Work The contribution process works off standard git Pull Requests . Every PR needs to be signed. The sign-off is a simple line at the end of the explanation for a commit. Your signature certifies that you wrote the patch or otherwise have the right to contribute the material. The rules are pretty simple, if you can certify the below (from developercertificate.org ): Developer Certificate of Origin Version 1 . 1 Copyright ( C ) 2004 , 2006 The Linux Foundation and its contributors . 1 Letterman Drive Suite D4700 San Francisco , CA , 94129 Everyone is permitted to copy and distribute verbatim copies of this license document , but changing it is not allowed . Developer ' s Certificate of Origin 1 . 1 By making a contribution to this project , I certify that : ( a ) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file ; or ( b ) The contribution is based upon previous work that , to the best of my knowledge , is covered under an appropriate open source license and I have the right under that license to submit that work with modifications , whether created in whole or in part by me , under the same open source license ( unless I am permitted to submit under a different license ), as indicated in the file ; or ( c ) The contribution was provided directly to me by some other person who certified ( a ), ( b ) or ( c ) and I have not modified it . ( d ) I understand and agree that this project and the contribution are public and that a record of the contribution ( including all personal information I submit with it , including my sign - off ) is maintained indefinitely and may be redistributed consistent with this project or the open source license ( s ) involved . Then you just add a line to every git commit message: Signed-off-by: John Doe <john.doe@example.com> Use your real name (sorry, no pseudonyms or anonymous contributions.) If you set your user.name and user.email git configs, you can sign your commit automatically with git commit -s . Note: If your git config information is set properly then viewing the git log information for your commit will look something like this: Author : John Doe < john . doe @ example . com > Date : Mon Oct 21 12 : 23 : 17 2019 - 0800 Update README Signed - off - by : John Doe < john . doe @ example . com > Notice the Author and Signed-off-by lines must match .","title":"Prerequisites"},{"location":"contributing-prerequisites/#before-submitting-your-operator","text":"First off, thanks for taking the time to contribute your Operator!","title":"Before submitting your Operator"},{"location":"contributing-prerequisites/#a-primer-to-kubernetes-community-operators","text":"This project collects Operators and publishes them to OperatorHub.io, a central registry for community-powered Kubernetes Operators. For OperatorHub.io your Operator needs to work with vanilla Kubernetes. This project also collects Community Operators that work with OpenShift to be displayed in the embedded OperatorHub. If you are new to Operators, start here .","title":"A primer to Kubernetes Community Operators"},{"location":"contributing-prerequisites/#sign-your-work","text":"The contribution process works off standard git Pull Requests . Every PR needs to be signed. The sign-off is a simple line at the end of the explanation for a commit. Your signature certifies that you wrote the patch or otherwise have the right to contribute the material. The rules are pretty simple, if you can certify the below (from developercertificate.org ): Developer Certificate of Origin Version 1 . 1 Copyright ( C ) 2004 , 2006 The Linux Foundation and its contributors . 1 Letterman Drive Suite D4700 San Francisco , CA , 94129 Everyone is permitted to copy and distribute verbatim copies of this license document , but changing it is not allowed . Developer ' s Certificate of Origin 1 . 1 By making a contribution to this project , I certify that : ( a ) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file ; or ( b ) The contribution is based upon previous work that , to the best of my knowledge , is covered under an appropriate open source license and I have the right under that license to submit that work with modifications , whether created in whole or in part by me , under the same open source license ( unless I am permitted to submit under a different license ), as indicated in the file ; or ( c ) The contribution was provided directly to me by some other person who certified ( a ), ( b ) or ( c ) and I have not modified it . ( d ) I understand and agree that this project and the contribution are public and that a record of the contribution ( including all personal information I submit with it , including my sign - off ) is maintained indefinitely and may be redistributed consistent with this project or the open source license ( s ) involved . Then you just add a line to every git commit message: Signed-off-by: John Doe <john.doe@example.com> Use your real name (sorry, no pseudonyms or anonymous contributions.) If you set your user.name and user.email git configs, you can sign your commit automatically with git commit -s . Note: If your git config information is set properly then viewing the git log information for your commit will look something like this: Author : John Doe < john . doe @ example . com > Date : Mon Oct 21 12 : 23 : 17 2019 - 0800 Update README Signed - off - by : John Doe < john . doe @ example . com > Notice the Author and Signed-off-by lines must match .","title":"Sign Your Work"},{"location":"contributing-via-pr/","text":"Submitting your Operator via Pull Requests (PR) in community operators project Overview To submit an operator one has to do these steps Test your operator locally Fork project https://github.com/operator-framework/community-operators Make a pull request Place the operator in the target directory. More info community-operators (Openshift operator) upstream-community-operators (Kubernetes operator) Conigure ci.yaml file. More info Setup reviewers Operator versioning strategy Verify tests and fix problems, if possible Ask for help in the PR in case of problems Test locally before you contribute The team behind OperatorHub.io will support you in making sure your Operator works and is packaged correctly. You can accelerate your submission greatly by testing your Operator with the Operator Framework by following our documentation for local manual testing or automated testing using scripts . You are responsible for testing your Operator's APIs when deployed with OLM. Pull request When a pull request is created, a number of tests are executed. One can see the results in Checks tab. Verify CI test results Every PR against this repository is tested via Continuous Integration . During these tests your Operator will be deployed on either a minikube or OpenShift 4 environments and checked for a healthy deployment. Also several tools are run to check your bundle for completeness. These are the same tools as referenced in our testing docs and testing scripts . Pay attention to the result of GitHub checks. More detailed information about our Continuous Integration process can be found here Test results Test results are located in Checks tab. Then they can be found in Operator test list on left side. Once clicked on it the summary of test will be shown. There are multiple tests. For easy mapping different fruit names were chosen. Look at our testing suite for more information. One can see more details about tests when clicking on directly on them. Test on an Openshift cluster For an Openshift operator (operators in community-operators directory), the deployment of operator is executed on an Openshift cluster via ci/prow/deploy-operator-on-openshift . Note The kiwi test does not include the deployment test on k8s cluster . This can be forced by specifying label test/force-deploy-on-kubernetes in the PR. You are done User is done when all tests are green. When the PR is merged, on can follow process explained in Release pipeline . Test results failed? When operator tests are failing, one can see following picture In case of failures, please have a look at logs of specific tests. If error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it.","title":"Creating pull request (PR)"},{"location":"contributing-via-pr/#submitting-your-operator-via-pull-requests-pr-in-community-operators-project","text":"","title":"Submitting your Operator via Pull Requests (PR) in community operators project"},{"location":"contributing-via-pr/#overview","text":"To submit an operator one has to do these steps Test your operator locally Fork project https://github.com/operator-framework/community-operators Make a pull request Place the operator in the target directory. More info community-operators (Openshift operator) upstream-community-operators (Kubernetes operator) Conigure ci.yaml file. More info Setup reviewers Operator versioning strategy Verify tests and fix problems, if possible Ask for help in the PR in case of problems","title":"Overview"},{"location":"contributing-via-pr/#test-locally-before-you-contribute","text":"The team behind OperatorHub.io will support you in making sure your Operator works and is packaged correctly. You can accelerate your submission greatly by testing your Operator with the Operator Framework by following our documentation for local manual testing or automated testing using scripts . You are responsible for testing your Operator's APIs when deployed with OLM.","title":"Test locally before you contribute"},{"location":"contributing-via-pr/#pull-request","text":"When a pull request is created, a number of tests are executed. One can see the results in Checks tab.","title":"Pull request"},{"location":"contributing-via-pr/#verify-ci-test-results","text":"Every PR against this repository is tested via Continuous Integration . During these tests your Operator will be deployed on either a minikube or OpenShift 4 environments and checked for a healthy deployment. Also several tools are run to check your bundle for completeness. These are the same tools as referenced in our testing docs and testing scripts . Pay attention to the result of GitHub checks. More detailed information about our Continuous Integration process can be found here","title":"Verify CI test results"},{"location":"contributing-via-pr/#test-results","text":"Test results are located in Checks tab. Then they can be found in Operator test list on left side. Once clicked on it the summary of test will be shown. There are multiple tests. For easy mapping different fruit names were chosen. Look at our testing suite for more information. One can see more details about tests when clicking on directly on them.","title":"Test results"},{"location":"contributing-via-pr/#test-on-an-openshift-cluster","text":"For an Openshift operator (operators in community-operators directory), the deployment of operator is executed on an Openshift cluster via ci/prow/deploy-operator-on-openshift . Note The kiwi test does not include the deployment test on k8s cluster . This can be forced by specifying label test/force-deploy-on-kubernetes in the PR.","title":"Test on an Openshift cluster"},{"location":"contributing-via-pr/#you-are-done","text":"User is done when all tests are green. When the PR is merged, on can follow process explained in Release pipeline .","title":"You are done"},{"location":"contributing-via-pr/#test-results-failed","text":"When operator tests are failing, one can see following picture In case of failures, please have a look at logs of specific tests. If error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it.","title":"Test results failed?"},{"location":"contributing-where-to/","text":"Where to contribute There are 2 directories where you can contribute, depending on some basic requirements and where you would like your Operator to show up: Target Directory Appears on Target Platform Requirements community-operators Embedded OperatorHub in OpenShift 4 OpenShift / OKD needs to work on OpenShift 4 or newer upstream-community-operators OperatorHub.io Upstream Kubernetes needs to work on Kubernetes 1.7 or newer These repositories are used by OpenShift 4 and OperatorHub.io respectively. Specifically, Operators for Upstream Kubernetes for OperatorHub.io won't automatically appear on the embedded OperatorHub in OpenShift and should not be used on OpenShift. If your Operator works on both Kubernetes and OpenShift, place a copy of your packaged bundle in the upstream-community-operators directory, as well as the community-operators directory. Submit them as separate PRs. For partners and ISVs, certified operators can now be submitted via connect.redhat.com. If you have submitted your Operator there already, please ensure your submission here uses a different package name (refer to the README for more details).","title":"Where to place operator"},{"location":"contributing-where-to/#where-to-contribute","text":"There are 2 directories where you can contribute, depending on some basic requirements and where you would like your Operator to show up: Target Directory Appears on Target Platform Requirements community-operators Embedded OperatorHub in OpenShift 4 OpenShift / OKD needs to work on OpenShift 4 or newer upstream-community-operators OperatorHub.io Upstream Kubernetes needs to work on Kubernetes 1.7 or newer These repositories are used by OpenShift 4 and OperatorHub.io respectively. Specifically, Operators for Upstream Kubernetes for OperatorHub.io won't automatically appear on the embedded OperatorHub in OpenShift and should not be used on OpenShift. If your Operator works on both Kubernetes and OpenShift, place a copy of your packaged bundle in the upstream-community-operators directory, as well as the community-operators directory. Submit them as separate PRs. For partners and ISVs, certified operators can now be submitted via connect.redhat.com. If you have submitted your Operator there already, please ensure your submission here uses a different package name (refer to the README for more details).","title":"Where to contribute"},{"location":"operator-ci-yaml/","text":"Operator ci.yaml Each operator should have ci.yaml configuration file to be present in operator directory (for example community-operators/aqua/ci.yaml ). This configuration file is used by community-operators pipeline to setup various feature like reviewers or operator versioning . Note One can create ci.yaml file with new operator version when file is not already present. This operation can be done in the same PR with other operator changes. Warning If ci.yaml is already present, modification to the file will be only possible via an extra PR with single file modification. One can update list of reviewers or change version strategy. Reviewers It is required to setup reviewers in ci.yaml file. It can be done by adding reviewers tag with list of github usernames. For example Example $ cat <path-to-operator>/ci.yaml --- reviewers: - J0zi - mvalarh More advanced setup can be done via documentation here Operator versioning Operators have multiple versions. When a new version is released, OLM can update operator automatically. There are 2 update strategies possible, which are defined in ci.yaml at the operator top level. replaces-mode Every next version defines which version will be replaced using replaces key in the CSV file. It means, that there is a possibility to omit some versions from the update graph. Best practice is to put them to a separate channel then. semver-mode Every version will be replaced by next higher version according semantic versioning. Restrictions Contributor can decide, if semver-mode or replaces-mode mode will be used for a specific operator. By default, replaces-mode is activated, when ci.yaml file is present and contains updateGraph: replaces-mode . When a contributor decides to switch and use semver-mode , it will be specified in ci.yaml file or the file will be missing. Example $ cat <path-to-operator>/ci.yaml --- # Use `replaces-mode` or `semver-mode`. updateGraph: replaces-mode","title":"Operator ci.yaml"},{"location":"operator-ci-yaml/#operator-ciyaml","text":"Each operator should have ci.yaml configuration file to be present in operator directory (for example community-operators/aqua/ci.yaml ). This configuration file is used by community-operators pipeline to setup various feature like reviewers or operator versioning . Note One can create ci.yaml file with new operator version when file is not already present. This operation can be done in the same PR with other operator changes. Warning If ci.yaml is already present, modification to the file will be only possible via an extra PR with single file modification. One can update list of reviewers or change version strategy.","title":"Operator ci.yaml"},{"location":"operator-ci-yaml/#reviewers","text":"It is required to setup reviewers in ci.yaml file. It can be done by adding reviewers tag with list of github usernames. For example","title":"Reviewers"},{"location":"operator-ci-yaml/#operator-versioning","text":"Operators have multiple versions. When a new version is released, OLM can update operator automatically. There are 2 update strategies possible, which are defined in ci.yaml at the operator top level.","title":"Operator versioning"},{"location":"operator-overwrite/","text":"Orange test fails even when my operator is ok. There is one case when your operator is correct, but orange test might failt. This happened when some operator versions are already published and one wants to change some cosmetic changes to bundle or convert format from package manifest to bundle format. In tese scenarios one can follow instruction bellow Operator version overwrite Operator recreate Operator version overwrite When cosmetic changes are made to already published operator version Orange test will fail. In this case one needs to have allow/operator-version-overwrite label set. One can set it or ask maintainer to set it for you. After the PR will be merged, the following changes will happen Bundle for current operator version will be overwritten Build catalog with new bundle Operator recreate When a whole operator is recreated (usually when converting whole operator from packagemanifest format to bundle format). One needs to have allow/operator-recreate label set. One can set it or ask maintainer to set it for you. After the PR will be merged, the following changes will happen Delete operator Rebuild all bundles Build catalog with new bundles","title":"Updating published Operator version"},{"location":"operator-overwrite/#orange-test-fails-even-when-my-operator-is-ok","text":"There is one case when your operator is correct, but orange test might failt. This happened when some operator versions are already published and one wants to change some cosmetic changes to bundle or convert format from package manifest to bundle format. In tese scenarios one can follow instruction bellow Operator version overwrite Operator recreate","title":"Orange test fails even when my operator is ok."},{"location":"operator-overwrite/#operator-version-overwrite","text":"When cosmetic changes are made to already published operator version Orange test will fail. In this case one needs to have allow/operator-version-overwrite label set. One can set it or ask maintainer to set it for you. After the PR will be merged, the following changes will happen Bundle for current operator version will be overwritten Build catalog with new bundle","title":"Operator version overwrite"},{"location":"operator-overwrite/#operator-recreate","text":"When a whole operator is recreated (usually when converting whole operator from packagemanifest format to bundle format). One needs to have allow/operator-recreate label set. One can set it or ask maintainer to set it for you. After the PR will be merged, the following changes will happen Delete operator Rebuild all bundles Build catalog with new bundles","title":"Operator recreate"},{"location":"operator-release-process/","text":"Operator release Operator release workflow Release workflow contains all jobs. This can be found in action tab of the project. When operator is merged to master following scenarios will happen: For a k8s case (upstream-community-operators) Push to quay (to support old app registry) Build index image for k8s Build image image for operatorhub.io page Deploy operatorhub.io page For an Openshift case (community-operators) Push to quay (to support old app registry) Build index image for different Openshift versions (v4.6 and v4.7 in this case) and multiarch image is also is produced. Build image image for dev.operatorhub.io page (for development purposes only) Deploy dev.operatorhub.io page (for development purposes only) Operator is published After this process, your operator will be published. Index image location For Openshift: registry.redhat.io/redhat/community-operator-index:v4.6 registry.redhat.io/redhat/community-operator-index:v4.7 registry.redhat.io/redhat/community-operator-index:latest - this is a clone of v4.6 from historical reasons as it always was a clone of v4.6 . Will be deprecated in the future. For Kubernetes: quay.io/operatorhubio/catalog:latest Bundle images location For Openshift: quay.io/openshift-community-operators/ For Kubernetes: quay.io/operatorhubio/","title":"Release process"},{"location":"operator-release-process/#operator-release","text":"","title":"Operator release"},{"location":"operator-release-process/#operator-release-workflow","text":"Release workflow contains all jobs. This can be found in action tab of the project. When operator is merged to master following scenarios will happen:","title":"Operator release workflow"},{"location":"operator-release-process/#for-a-k8s-case-upstream-community-operators","text":"Push to quay (to support old app registry) Build index image for k8s Build image image for operatorhub.io page Deploy operatorhub.io page","title":"For a k8s case (upstream-community-operators)"},{"location":"operator-release-process/#for-an-openshift-case-community-operators","text":"Push to quay (to support old app registry) Build index image for different Openshift versions (v4.6 and v4.7 in this case) and multiarch image is also is produced. Build image image for dev.operatorhub.io page (for development purposes only) Deploy dev.operatorhub.io page (for development purposes only)","title":"For an Openshift case (community-operators)"},{"location":"operator-release-process/#operator-is-published","text":"After this process, your operator will be published.","title":"Operator is published"},{"location":"operator-test-suite/","text":"Operator tests Running tests One can run test by entering to 'community-operators' project directory and run with following command with these options. ' ' and ' ' options are optional cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ <test-type1,test-type2,...,test-typeN> \\ <operator-version-dir-relative-to-community-operators-project> \\ [<git repo>] [<git branch>] Test type List of tests are shown in following table : Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange Logs Logs can be found in /tmp/op-test/log.out Testing log files If operator test fails, one can enter to testing container via follwing command. One can substitue 'docker' with 'podman' when supported docker exec -it op-test /bin/bash Examples Running tests from local direcotry Following example will run 'all' tests on 'aqua' operator with version '1.0.2' from 'upstream-community-operators (k8s)' directory. 'community-operators' project will be taken from local directory one is running command from ($PWD). cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ all \\ upstream-community-operators/aqua/1.0.2 Running tests from official 'community-operators' repo Following example will run 'kiwi' and 'lemon' tests on 'aqua' operator with version '1.0.2' from 'community-operators (Openshift)' directory. 'community-operators' project will be taken from git repo 'https://github.com/operator-framework/community-operators' and 'master' branch cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ kiwi,lemon \\ community-operators/aqua/1.0.2 \\ https://github.com/operator-framework/community-operators \\ master Running tests from forked 'community-operators' repo ans specific branch Following example will run 'kiwi' and 'lemon' tests on 'kong' operator with version '0.5.0' from 'upstream-community-operators (k8s)' directory.'community-operators' project will be taken from git repo 'https://github.com/Kong/community-operators' and 'release/v0.5.0' branch ('https://github.com/Kong/community-operators/tree/release/v0.5.0') cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ kiwi,lemon \\ upstream-community-operators/kong/0.5.0 \\ https://github.com/Kong/community-operators \\ release/v0.5.0 Misc Name Description Default OP_TEST_DEBUG Debug level (0-3) 0 OP_TEST_CONTAINER_TOOL Container tool used on host docker OP_TEST_DRY_RUN Will print commands to be executed 0 Testing operators by Ansible Documentation for testing is located here","title":"Via test suite"},{"location":"operator-test-suite/#operator-tests","text":"","title":"Operator tests"},{"location":"operator-test-suite/#running-tests","text":"One can run test by entering to 'community-operators' project directory and run with following command with these options. ' ' and ' ' options are optional cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ <test-type1,test-type2,...,test-typeN> \\ <operator-version-dir-relative-to-community-operators-project> \\ [<git repo>] [<git branch>]","title":"Running tests"},{"location":"operator-test-suite/#examples","text":"","title":"Examples"},{"location":"operator-test-suite/#running-tests-from-local-direcotry","text":"Following example will run 'all' tests on 'aqua' operator with version '1.0.2' from 'upstream-community-operators (k8s)' directory. 'community-operators' project will be taken from local directory one is running command from ($PWD). cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ all \\ upstream-community-operators/aqua/1.0.2","title":"Running tests from local direcotry"},{"location":"operator-test-suite/#running-tests-from-official-community-operators-repo","text":"Following example will run 'kiwi' and 'lemon' tests on 'aqua' operator with version '1.0.2' from 'community-operators (Openshift)' directory. 'community-operators' project will be taken from git repo 'https://github.com/operator-framework/community-operators' and 'master' branch cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ kiwi,lemon \\ community-operators/aqua/1.0.2 \\ https://github.com/operator-framework/community-operators \\ master","title":"Running tests from official 'community-operators' repo"},{"location":"operator-test-suite/#running-tests-from-forked-community-operators-repo-ans-specific-branch","text":"Following example will run 'kiwi' and 'lemon' tests on 'kong' operator with version '0.5.0' from 'upstream-community-operators (k8s)' directory.'community-operators' project will be taken from git repo 'https://github.com/Kong/community-operators' and 'release/v0.5.0' branch ('https://github.com/Kong/community-operators/tree/release/v0.5.0') cd <community-operators> bash <(curl -sL https://cutt.ly/WhkV76k) \\ kiwi,lemon \\ upstream-community-operators/kong/0.5.0 \\ https://github.com/Kong/community-operators \\ release/v0.5.0","title":"Running tests from forked 'community-operators' repo ans specific branch"},{"location":"operator-test-suite/#misc","text":"Name Description Default OP_TEST_DEBUG Debug level (0-3) 0 OP_TEST_CONTAINER_TOOL Container tool used on host docker OP_TEST_DRY_RUN Will print commands to be executed 0","title":"Misc"},{"location":"operator-test-suite/#testing-operators-by-ansible","text":"Documentation for testing is located here","title":"Testing operators by Ansible"},{"location":"operator-version-update/","text":"Updating existing Operators Updating operator version Note It is strongly recommended to bump operator version if possible. Operator version update can be done by creating new directory with version name in operator dir without ' v '. For example updating aqua operator from 5.3.0 to 6.0.0 $ tree community-operators/aqua/ -d community-operators/aqua/ \u251c\u2500\u2500 0 .0.1 \u251c\u2500\u2500 0 .0.2 \u251c\u2500\u2500 1 .0.0 \u251c\u2500\u2500 1 .0.1 \u251c\u2500\u2500 1 .0.2 \u251c\u2500\u2500 5 .3.0 \u2514\u2500\u2500 6 .0.0 Minor (cosmetics) changes There are some case when only some minor changes to the existing operator are needed (like description update or an update of icon). In this case pipeline will set coresponding label and automatically handle such case. Allowed changes Only changes in csv (*.clusterserviceversion.yaml) are allowed List of allowed tag changes in csv spec.description spec.DisplayName spec.icon Operator versioning strategy Warning Updating existing ci.yaml is only possible via an extra PR with single file modification. Otherwise tests will fail Sometimes it is needed to change how operator versions are built in to the index. This can be controlled by ci.yaml file. More info Reviewers update Warning Updating existing ci.yaml is only possible via an extra PR with single file modification. Otherwise tests will fail While operator is involving over a time, some time it is needed to change reviewers. This can be controlled by ci.yaml file. More info","title":"Updating existing Operators"},{"location":"operator-version-update/#updating-existing-operators","text":"","title":"Updating existing Operators"},{"location":"operator-version-update/#updating-operator-version","text":"Note It is strongly recommended to bump operator version if possible. Operator version update can be done by creating new directory with version name in operator dir without ' v '. For example updating aqua operator from 5.3.0 to 6.0.0 $ tree community-operators/aqua/ -d community-operators/aqua/ \u251c\u2500\u2500 0 .0.1 \u251c\u2500\u2500 0 .0.2 \u251c\u2500\u2500 1 .0.0 \u251c\u2500\u2500 1 .0.1 \u251c\u2500\u2500 1 .0.2 \u251c\u2500\u2500 5 .3.0 \u2514\u2500\u2500 6 .0.0","title":"Updating operator version"},{"location":"operator-version-update/#minor-cosmetics-changes","text":"There are some case when only some minor changes to the existing operator are needed (like description update or an update of icon). In this case pipeline will set coresponding label and automatically handle such case.","title":"Minor (cosmetics) changes"},{"location":"operator-version-update/#operator-versioning-strategy","text":"Warning Updating existing ci.yaml is only possible via an extra PR with single file modification. Otherwise tests will fail Sometimes it is needed to change how operator versions are built in to the index. This can be controlled by ci.yaml file. More info","title":"Operator versioning strategy"},{"location":"operator-version-update/#reviewers-update","text":"Warning Updating existing ci.yaml is only possible via an extra PR with single file modification. Otherwise tests will fail While operator is involving over a time, some time it is needed to change reviewers. This can be controlled by ci.yaml file. More info","title":"Reviewers update"},{"location":"packaging-operator/","text":"Package your Operator This repository makes use of the Operator Framework and its packaging concept for Operators. Your contribution is welcome in the form of a Pull Request with your Operator packaged for use with Operator Lifecycle Manager . Packaging format Your Operator submission can be formatted either following the packagemanifest or the newer bundle format. The former allows to ship your entire Operator with all its versions in one single directory. The latter allows shipping individual releases in container images. Both are supported but mixing of formats within a single Operator is not allowed. You need to decide for one or the other for your entire Operator listing. In general a released version of your Operator is described in a ClusterServiceVersion manifest alongside the CustomResourceDefinitions of your Operator and additional metadata describing your Operator listing. Create a ClusterServiceVersion To add your operator to any of the supported platforms, you will need to submit metadata for your Operator to be used by the Operator Lifecycle Manager (OLM). This is YAML file called ClusterServiceVersion which contains references to all of the CRDs, RBAC rules, Deployment and container image needed to install and securely run your Operator. It also contains user-visible info like a description of its features and supported Kubernetes versions. Note that your Operator's CRDs are shipped in separate manifests alongside the CSV so OLM can register them during installation (your Operator is not supposed to self-register its CRDs). Follow this guide to create an OLM-compatible CSV for your operator. You can also see an example here . An Operator's CSV must contain the fields mentioned here for it to be displayed properly within the various platforms. If your operator needs new category, follow the instructions about categories . There is one CSV per version of your Operator alongside the CRDs. Create a release using the packagemanifest format The packagemanifest format is a directory structure in which the top-level directory represents your Operator as a package . Below that top-level directory are versioned sub-directories, one for each a released version of your Operator. The sub-directory names follow semantic version of your Operator and contain the CustomResourceDefinition s and ClusterServiceVersion . The exact version is the one of your Operator as defined in spec.version inside the CSV. The version should also be reflected in the CSV file name for ease of use. It is required that the spec.name field in the CSV is also the same as the package name. Follow the example below, assuming your Operator package is called my-operator : $ tree my-operator/ my-operator \u251c\u2500\u2500 0 .1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0 .5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1 .0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml The package.yaml is a YAML file at the root level of the package directory. It provides the package name, a selection of channels pointing to potentially different Operator Versions/CSVs and a default channel. The package name is what users on cluster see when they discover Operators available to install. Use channels to allow your users to select a different update cadence, e.g. stable vs. nightly . If you have only a single channel the use of defaultChannel is optional. An example of my-operator.package.yaml : packageName : my-operator channels : - name : stable currentCSV : my-operator.v1.0.0 - name : nightly currentCSV : my-operator.v1.0.3-beta defaultChannel : stable Your CSV versioning should follow semantic versioning concepts. Again, packageName , the suffix of the package.yaml file name and the field in spec.name in the CSV should all refer to the same Operator name. Create a release using the bundle format Alternatively you can use the bundle format, which is nowadays also the default of the Operator-SDK. The bundle format has a top-level directory named after your Operator name in the ClusterServiceVersion directory. Inside are sub-directories for the individual bundle, named after the semantic versioning release of your Operator. Unlike the packagemanifest format all metadata is defined within the individual release of the Operator. That is, inside each bundle. This includes the channel definitions. Rather than describing the list of available release channels in a top-level package.yaml it is compiled dynamically when a bundle is added. The default channel is also defined within the bundle and overwritten by every new bundle you add (this is a known limitation and is being worked on). Within each version you have your CustomResourceDefinitions , ClusterServiceVersion file (containing the same name and version of your Operator as defined inside the YAML structure) and some metadata about the bundle. You can learn more about the bundle format here and also see some examples . Your directory structure might look like this when using the bundle format. Notice that the Dockerfile is optionally and actually ignored. The processing pipeline of this site builds a container image for each of your bundle regardless. $ tree my-operator/ my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 2.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... If you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle . Moving from packagemanifest to bundle format Eventually this repository will only accept bundle format at some point in the future. Also the bundle format has more features like semver mode or, in the future, installing bundles directly outside of a catalog. Migration of existing content, irregardless of whether the Operator was created with the SDK, can be achieved with the opm tool on per Operator version basis. You can download opm here . Suppose v2.0.0 is the version of the Operator you want to test convert to bundle format directory with the opm tool: mkdir /tmp/my-operator-2.0.0-bundle/ cd /tmp/my-operator-2.0.0-bundle/ opm alpha bundle build --directory /path/to/my-operator/2.0.0-bundle/ --tag my-operator-bundle:v2.0.0 --output-dir . This will have generated the bundle format layout in the current working directory /tmp/my-operator-2.0.0-bundle/ : $ tree . /tmp/my-operator-2.0.0-bundle/ \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 bundle.Dockerfile You can verify the generated bundle metadata for semantic correctness with the the operator-sdk on this directory. operator-sdk bundle validate /tmp/my-operator-2.0.0-bundle/ --select-optional name=operatorhub You can download operator-sdk here . Operator icon Icon is defined in a CSV as spec.icon . If you don't have own icon, you should use default one: icon: - base64data: \"PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNTguNTEgMjU4LjUxIj48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2QxZDFkMTt9LmNscy0ye2ZpbGw6IzhkOGQ4Zjt9PC9zdHlsZT48L2RlZnM+PHRpdGxlPkFzc2V0IDQ8L3RpdGxlPjxnIGlkPSJMYXllcl8yIiBkYXRhLW5hbWU9IkxheWVyIDIiPjxnIGlkPSJMYXllcl8xLTIiIGRhdGEtbmFtZT0iTGF5ZXIgMSI+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMTI5LjI1LDIwQTEwOS4xLDEwOS4xLDAsMCwxLDIwNi40LDIwNi40LDEwOS4xLDEwOS4xLDAsMSwxLDUyLjExLDUyLjExLDEwOC40NSwxMDguNDUsMCwwLDEsMTI5LjI1LDIwbTAtMjBoMEM1OC4xNiwwLDAsNTguMTYsMCwxMjkuMjVIMGMwLDcxLjA5LDU4LjE2LDEyOS4yNiwxMjkuMjUsMTI5LjI2aDBjNzEuMDksMCwxMjkuMjYtNTguMTcsMTI5LjI2LTEyOS4yNmgwQzI1OC41MSw1OC4xNiwyMDAuMzQsMCwxMjkuMjUsMFoiLz48cGF0aCBjbGFzcz0iY2xzLTIiIGQ9Ik0xNzcuNTQsMTAzLjQxSDE0MS42NkwxNTQuOSw2NS43NmMxLjI1LTQuNC0yLjMzLTguNzYtNy4yMS04Ljc2SDEwMi45M2E3LjMyLDcuMzIsMCwwLDAtNy40LDZsLTEwLDY5LjYxYy0uNTksNC4xNywyLjg5LDcuODksNy40LDcuODloMzYuOUwxMTUuNTUsMTk3Yy0xLjEyLDQuNDEsMi40OCw4LjU1LDcuMjQsOC41NWE3LjU4LDcuNTgsMCwwLDAsNi40Ny0zLjQ4TDE4NCwxMTMuODVDMTg2Ljg2LDEwOS4yNCwxODMuMjksMTAzLjQxLDE3Ny41NCwxMDMuNDFaIi8+PC9nPjwvZz48L3N2Zz4=\" mediatype: \"image/svg+xml\" Supported formats: svg, jpg, png Updating your existing Operator Unless of purely cosmectic nature, subsequent updates to your Operator should result in new bundle directories being added, containing an updated CSV as well as copied, updated and/or potentially newly added CRDs. Within your new CSV, update the spec.version field to the desired new semantic version of your Operator. In order to have OLM enable updates to your new Operator version you can choose between three update modes: semver-mode , semver-skippatch-mode and replaces-mode . The default is semver-mode . If you want to change the default, place a file called ci.yaml in your top-level directory (works for both packagemanifest or bundle format) and set it to either of the two other values. For example: updateGraph : replaces-mode semver-mode OLM treats all your Operator versions with semantic version rules and update them in order of those versions. That is, every version will be replaced by the next higher version according semantic versioning sort order. During an update on the cluster OLM will update all the way to the latest version, one version at a time. To use this, simply specify spec.version in your CSV. If you accidentally add spec.replaces this will contradict semantic versioning and raise an error. semver-skippatch Works like semver with a slightly different behavior of OLM on cluster, where instead of updating from e.g. 1.1.0 and an update path according to semver ordering rules like so: 1.1.0 -> 1.1.1 -> 1.1.2 , the update would jump straight to 1.1.2 instead of updating to 1.1.1 first. replaces-mode Each Operator bundle not only contains spec.version but also points to an older version it can upgrade from via spec.replaces key in the CSV file, e.g. replaces: my-operator.v1.0.0 . From this chain of back pointers OLM computes the update graph at runtime. This allows to omit some versions from the update graph or release special leaf versions. Regardless of which mode you choose to have OLM create update paths for your Operator, it continuous update your Operator often as new features are added and bugs are fixed. Operator Bundle Editor You can now create your Operator bundle using the bundle editor . Starting by uploading your Kubernetes YAML manifests, the forms on the page will be populated with all valid information and used to create the new Operator bundle. You can modify or add properties through these forms as well. The result will be a downloadable ZIP file. Provide information about your Operator A large part of the information gathered in the CSV is used for user-friendly visualization on OperatorHub.io or components like the embedded OperatorHub in OpenShift. Your work is on display, so please ensure to provide relevant information in your Operator's description, specifically covering: What the managed application is about and where to find more information The features your Operator and how to use it Any manual steps required to fulfill pre-requisites for running / installing your Operator","title":"Operator structure"},{"location":"packaging-operator/#package-your-operator","text":"This repository makes use of the Operator Framework and its packaging concept for Operators. Your contribution is welcome in the form of a Pull Request with your Operator packaged for use with Operator Lifecycle Manager .","title":"Package your Operator"},{"location":"packaging-operator/#provide-information-about-your-operator","text":"A large part of the information gathered in the CSV is used for user-friendly visualization on OperatorHub.io or components like the embedded OperatorHub in OpenShift. Your work is on display, so please ensure to provide relevant information in your Operator's description, specifically covering: What the managed application is about and where to find more information The features your Operator and how to use it Any manual steps required to fulfill pre-requisites for running / installing your Operator","title":"Provide information about your Operator"},{"location":"packaging-required-fields/","text":"Required fields within your CSV Preparing your CSV for use with OLM Before you begin, we strongly advise that you follow Operator-Lifecycle-Manager's docs on building a CSV for the Operator Framework . These outline the functional purpose of the CSV and which fields are required for installing your Operator CSV through OLM. Required fields for OperatorHub An Operator's CSV must contain the following fields and annotations for it to be displayed properly within OperatorHub.io and OperatorHub in OCP. Below is a guideline explaining each field, and at the bottom of this document is a full example of such a CSV. metadata : annotations : capabilities : One of the following : Basic Install, Seamless Upgrades, Full Lifecycle, Deep Insights, Auto Pilot. For more information see https://www.operatorhub.io/images/capability-level-diagram.svg categories : A comma separated list of categories from the values below. If not set, this will be set to \"Other\" in the UI containerImage : The repository that hosts the operator image. The format should match ${REGISTRYHOST}/${USERNAME}/${NAME}:${TAG} createdAt : The date that the operator was created. The format should match yyyy-mm-ddThh:mm:ssZ support : The name of the individual, company, or service that maintains this operator repository : (Optional) a URL to a source code repository of the Operator, intended for community Operators to direct users where to file issues / bug alm-examples : A string of a JSON list of example CRs for the operator's CRDs description : |- A short description of the operator that will be displayed on the marketplace tile If this annotation is not present, the `spec.description` value will be shown instead In either case, only the first 135 characters will appear spec : displayName : A short, readable name for the operator description : A detailed description of the operator, preferably in markdown format icon : - base64data : A base 64 representation of an image or logo associated with your operator mediatype : One of the following : image/png, image/jpeg, image/gif, image/svg+xml version : The operator version in semver format maintainers : - name : The name of the individual, company, or service that maintains this operator email : Email to reach maintainer provider : name : The name of the individual, company, or service that provides this operator links : - name : Title of the link (ex : Blog, Source Code etc.) url : url/link keywords : - 'A list of words that relate to your operator' - 'These are used when searching for operators in the UI' Logo requirements The logo for your Operator is inlined into the CSV as a base64-encoded string. The height:width ratio should be 1:2. The maximum dimensions are 80px for width and 40px in height. Categories For the best user experience, choose from the categories . If none of these categories fit your operator, please open a separate PR against categories.json . Once merged, you can open a PR with your operator assigned to your new category. Example CSV Below is an example of what the descheduler CSV may look like if it contained the expected annotations: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : capabilities : Seamless Upgrades categories : \"OpenShift Optional\" containerImage : registry.svc.ci.openshift.org/openshift/origin-v4.0:descheduler-operator createdAt : 2019-01-01T11:59:59Z description : An operator to run the OpenShift descheduler repository : https://github.com/openshift/descheduler-operator alm-examples : | [ { \"apiVersion\": \"descheduler.io/v1alpha1\", \"kind\": \"Descheduler\", \"metadata\": { \"name\": \"example-descheduler-1\" }, \"spec\": { \"schedule\": \"*/1 * * * ?\", \"strategies\": [ { \"name\": \"lownodeutilization\", \"params\": [ { \"name\": \"cputhreshold\", \"value\": \"10\" }, { \"name\": \"memorythreshold\", \"value\": \"20\" }, { \"name\": \"memorytargetthreshold\", \"value\": \"30\" } ] } ] } } ] ... ... ... spec : displayName : Descheduler description : |- # Descheduler for Kubernetes ## Introduction Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. The scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. The scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears first time for scheduling. As Kubernetes clusters are very dynamic and their state change over time, there may be desired to move already running pods to some other nodes for various reasons * Some nodes are under or over utilized. * The original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more. * Some nodes failed and their pods moved to other nodes. New nodes are added to clusters. Consequently, there might be several pods scheduled on less desired nodes in a cluster. Descheduler, based on its policy, finds pods that can be moved and evicts them. Please note, in current implementation, descheduler does not schedule replacement of evicted pods but relies on the default scheduler for that. ## Note Any api could be changed any time without any notice. That said, your feedback is very important and appreciated to make this project more stable and useful. icon : - base64data : this+is+a+base64-string== mediatype : image/png version : 0.0.1 provider : name : Red Hat, Inc. maintainers : - email : support@redhat.com name : Red Hat links : - name : GitHub Repository url : https://github.com/openshift/descheduler-operator keywords : [ 'deschedule' , 'scale' , 'binpack' , 'efficiency' ] ... ... ...","title":"Required fields"},{"location":"packaging-required-fields/#required-fields-within-your-csv","text":"","title":"Required fields within your CSV"},{"location":"packaging-required-fields/#preparing-your-csv-for-use-with-olm","text":"Before you begin, we strongly advise that you follow Operator-Lifecycle-Manager's docs on building a CSV for the Operator Framework . These outline the functional purpose of the CSV and which fields are required for installing your Operator CSV through OLM.","title":"Preparing your CSV for use with OLM"},{"location":"packaging-required-fields/#required-fields-for-operatorhub","text":"An Operator's CSV must contain the following fields and annotations for it to be displayed properly within OperatorHub.io and OperatorHub in OCP. Below is a guideline explaining each field, and at the bottom of this document is a full example of such a CSV. metadata : annotations : capabilities : One of the following : Basic Install, Seamless Upgrades, Full Lifecycle, Deep Insights, Auto Pilot. For more information see https://www.operatorhub.io/images/capability-level-diagram.svg categories : A comma separated list of categories from the values below. If not set, this will be set to \"Other\" in the UI containerImage : The repository that hosts the operator image. The format should match ${REGISTRYHOST}/${USERNAME}/${NAME}:${TAG} createdAt : The date that the operator was created. The format should match yyyy-mm-ddThh:mm:ssZ support : The name of the individual, company, or service that maintains this operator repository : (Optional) a URL to a source code repository of the Operator, intended for community Operators to direct users where to file issues / bug alm-examples : A string of a JSON list of example CRs for the operator's CRDs description : |- A short description of the operator that will be displayed on the marketplace tile If this annotation is not present, the `spec.description` value will be shown instead In either case, only the first 135 characters will appear spec : displayName : A short, readable name for the operator description : A detailed description of the operator, preferably in markdown format icon : - base64data : A base 64 representation of an image or logo associated with your operator mediatype : One of the following : image/png, image/jpeg, image/gif, image/svg+xml version : The operator version in semver format maintainers : - name : The name of the individual, company, or service that maintains this operator email : Email to reach maintainer provider : name : The name of the individual, company, or service that provides this operator links : - name : Title of the link (ex : Blog, Source Code etc.) url : url/link keywords : - 'A list of words that relate to your operator' - 'These are used when searching for operators in the UI'","title":"Required fields for OperatorHub"},{"location":"packaging-required-fields/#example-csv","text":"Below is an example of what the descheduler CSV may look like if it contained the expected annotations: apiVersion : operators.coreos.com/v1alpha1 kind : ClusterServiceVersion metadata : annotations : capabilities : Seamless Upgrades categories : \"OpenShift Optional\" containerImage : registry.svc.ci.openshift.org/openshift/origin-v4.0:descheduler-operator createdAt : 2019-01-01T11:59:59Z description : An operator to run the OpenShift descheduler repository : https://github.com/openshift/descheduler-operator alm-examples : | [ { \"apiVersion\": \"descheduler.io/v1alpha1\", \"kind\": \"Descheduler\", \"metadata\": { \"name\": \"example-descheduler-1\" }, \"spec\": { \"schedule\": \"*/1 * * * ?\", \"strategies\": [ { \"name\": \"lownodeutilization\", \"params\": [ { \"name\": \"cputhreshold\", \"value\": \"10\" }, { \"name\": \"memorythreshold\", \"value\": \"20\" }, { \"name\": \"memorytargetthreshold\", \"value\": \"30\" } ] } ] } } ] ... ... ... spec : displayName : Descheduler description : |- # Descheduler for Kubernetes ## Introduction Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. The scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. The scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears first time for scheduling. As Kubernetes clusters are very dynamic and their state change over time, there may be desired to move already running pods to some other nodes for various reasons * Some nodes are under or over utilized. * The original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more. * Some nodes failed and their pods moved to other nodes. New nodes are added to clusters. Consequently, there might be several pods scheduled on less desired nodes in a cluster. Descheduler, based on its policy, finds pods that can be moved and evicts them. Please note, in current implementation, descheduler does not schedule replacement of evicted pods but relies on the default scheduler for that. ## Note Any api could be changed any time without any notice. That said, your feedback is very important and appreciated to make this project more stable and useful. icon : - base64data : this+is+a+base64-string== mediatype : image/png version : 0.0.1 provider : name : Red Hat, Inc. maintainers : - email : support@redhat.com name : Red Hat links : - name : GitHub Repository url : https://github.com/openshift/descheduler-operator keywords : [ 'deschedule' , 'scale' , 'binpack' , 'efficiency' ] ... ... ...","title":"Example CSV"},{"location":"pull_request_template/","text":"Thanks submitting your Operator. Please check below list before you create your Pull Request. New Submissions [ ] Does your operator have nested directory structure ? [ ] Have you selected the Project Community Operator Submissions in your PR on the right-hand menu bar? [ ] Are you familiar with our contribution guidelines ? [ ] Have you packaged and deployed your Operator for Operator Framework? [ ] Have you tested your Operator with all Custom Resource Definitions? [ ] Have you tested your Operator in all supported installation modes ? [ ] Have you considered whether you want use semantic versioning order ? [ ] Is your submission signed ? [ ] Is operator icon set? Updates to existing Operators [ ] Did you create a ci.yaml file according to the update instructions ? [ ] Is your new CSV pointing to the previous version with the replaces property if you chose replaces-mode via the updateGraph property in ci.yaml ? [ ] Is your new CSV referenced in the appropriate channel defined in the package.yaml or annotations.yaml ? [ ] Have you tested an update to your Operator when deployed via OLM? [ ] Is your submission signed ? Your submission should not [ ] Modify more than one operator [ ] Modify an Operator you don't own [ ] Rename an operator - please remove and add with a different name instead [ ] Submit operators to both upstream-community-operators and community-operators at once [ ] Modify any files outside the above mentioned folders [ ] Contain more than one commit. Please squash your commits. Operator Description must contain (in order) [ ] Description about the managed Application and where to find more information [ ] Features and capabilities of your Operator and how to use it [ ] Any manual steps about potential pre-requisites for using your Operator Operator Metadata should contain [ ] Human readable name and 1-liner description about your Operator [ ] Valid category name 1 [ ] One of the pre-defined capability levels 2 [ ] Links to the maintainer, source code and documentation [ ] Example templates for all Custom Resource Definitions intended to be used [ ] A quadratic logo Remember that you can preview your CSV here . -- 1 If you feel your Operator does not fit any of the pre-defined categories, file an issue against this repo and explain your need 2 For more information see here","title":"Pull request template"},{"location":"testing-operators/","text":"Testing your Operator with Operator Framework Overview These instructions walk you through how to manually test that your Operator deploys correctly with Operator Framework, when packaged for the Operator Lifecycle Manager. Although your submission will always be tested as part of the CI you can accelerate the process by testing locally. The tests described in this document can also be executed automatically in a single step using a test suite A previous version of this document required quay.io, operator-courier and operator-marketplace to conduct the tests. This is no longer required. Accepted Contribution formats The process below assumes that you have a Kubernetes Operator in either of the two following formats supported by the Operator Framework: packagemanifest format (mandatory for all OLM versions prior to 0.14.0 and earlier, supported on all available versions) $ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml bundle format (supported with 0.14.0 or newer) $ tree my-operator/ my-operator \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 1.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... In both examples above my-operator is the name of your Operator which is available in 3 versions: 0.1.0 , 0.5.0 and 1.0.0 . If you are new to this or you don't have this format yet, refer to our contribution documentation . We will refer to both formats distinctively below where required. Mixing packagemanifest style and bundle format style Operator versions in a single Operator package is not supported . All versions all need to be in either one or the other format. Pre-Requisites Kubernetes cluster For \" upstream-community \" operators targeting Kubernetes and OperatorHub.io : * A running Kubernetes cluster; minikube or Kubernetes-in-Docker is the simplest approach For \" community \" operators targeting OCP/OKD and OperatorHub on OpenShift: * access to a running production-like OpenShift 4 cluster, use try.openshift.com to get a cluster on infrastructure of your choice * or access to all-in-one OpenShift 4 cluster, use CodeReady Containers to get a cluster on your local machine Container Tooling You need an OCI compatible container toolchain on the system where you test your Operator. Support options are: podman buildah moby (aka docker-ce aka docker ) Operator Framework components The following parts of the framework are used throughout the process and should be downloaded and put in your executable search path (Linux and Mac are supported): opm operator-sdk Preparing file structure Finally, if you haven't already done so, please clone this repository as well and create a branch: git clone https://github.com/operator-framework/community-operators.git cd community-operators/ git branch my-operator For simplicity (and if your Operator has dependencies on other community Operators) put your my-operator directory in either of the community-operators (for OpenShift's OperatorHub) or upstream-community-operators (for OperatorHub.io) directory (or both). cp -R my-operator/ community-operators/upstream-community-operators/ The name of the directory my-operator/ needs to match the Operator package name (without the slash) in either package.yaml (if you are using the packagemanifest format) or the container image label operators.operatorframework.io.bundle.package.v1 in the Dockerfile and annotations.yaml (if you are using the bundle format). If you are just adding a new version of your Operator, please create a subdirectory following semver conventions in your existing package directory, for example: cp -R my-operator/2.0.0 community-operators/upstream-community-operators/my-operator/ If you are using the packagemanifest format, don't forget to update the package.yaml file in the top-level directory to point to your new version/channels. Operator Metadata Validation If you are using packagemanifest format you will need to convert your metadata to bundle format for the validation step. In theory the previous operator-courier tool still works but it is no longer maintained. Temporary conversion of packagemanifest to bundle Suppose v2.0.0 is the version of the Operator you want to test convert to bundle format directory with the opm tool: mkdir /tmp/my-operator-2.0.0-bundle/ cd /tmp/my-operator-2.0.0-bundle/ opm alpha bundle build --directory /path/to/my-operator/2.0.0/ --tag my-operator-bundle:v2.0.0 --output-dir . This will have generated the bundle format layout in the current working directory /tmp/my-operator-2.0.0-bundle/ : $ tree . /tmp/my-operator-2.0.0-bundle/ \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 bundle.Dockerfile Run the following validation command of the operator-sdk from within this directory. operator-sdk bundle validate /tmp/my-operator-2.0.0-bundle/ --select-optional name=operatorhub Using operator-sdk to validate your Operator Validation using operator-sdk is only supported using the bundle format layout. See the previous step if you need to convert from packagemanifest . Validation is done on a per-Operator version basis. If you are not already in the Operator versions directory, switch to it now, e.g. cd my-operator/2.0.0/ With the Operator in bundle format use the operator-sdk to validate your bundle with the additional rules for community submissions: operator-sdk bundle validate --select-optional name=operatorhub . The output might look similar to this: INFO[0000] Found annotations file bundle-dir=. container-tool=docker INFO[0000] Could not find optional dependencies file bundle-dir=. container-tool=docker INFO[0000] All validation tests have completed successfully If there are any errors or warnings they will be displayed there. The container-tool will be automatically determined given your environment. If you want to force to use podman instead of docker , supply the -b switch to the operator-sdk bundle validate command . Any warnings here might turn into failing pipeline tests here. Please correct all issues displayed. A list of fields that are scanned can also be reviewed with this list . Publishing your Operator metadata to a catalog for testing Building a catalog using packagemanifest format When your Operator metadata is formatted in packagemanifest layout you need to place it in the directory structure of the community-operators repository, according to pre-requisites step . For example, assuming version 2.0.0 is the version you like to test: $ tree upstream-community-operators/ upstream-community-operators/ \u2502 ... \u2502 \u2514\u2500\u2500my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 2.0.0 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u2514\u2500\u2500 my-operator.package.yaml You can build a catalog for OLM containing either all Operators or just yours with a Dockerfile like this FROM quay.io/operator-framework/upstream-registry-builder as builder COPY upstream-community-operators manifests RUN /bin/initializer -o ./bundles.db FROM scratch COPY --from = builder /etc/nsswitch.conf /etc/nsswitch.conf COPY --from = builder /bundles.db /bundles.db COPY --from = builder /bin/registry-server /registry-server COPY --from = builder /bin/grpc_health_probe /bin/grpc_health_probe EXPOSE 50051 ENTRYPOINT [ \"/registry-server\" ] CMD [ \"--database\" , \"bundles.db\" ] Simply adjust the second line to either include all OpenShift Community Operators ( community-operators ), or all OperatorHub.io Operators ( upstream-community-operators ) or just your Operator (e.g. upstream-community-operator/my-operator ). Place the Dockerfile in the top-level directory of your cloned copy of this repo, build it and push to a registry from where you can download it to your Kubernetes cluster later. For example: podman build -f catalog.Dockerfile -t my-test-catalog:latest . podman tag my-test-catalog:latest quay.io/myaccount/my-test-catalog:latest podman push quay.io/myaccount/my-test-catalog:latest Building a catalog using bundles When your Operator metadata is formatted in bundle layout you can optionally add it to the existing directory structure like described in the pre-requisites step . For building a catalog this is not required because with Operator bundles versions are incrementally added to an existing or empty catalog. For example, assuming version 2.0.0 is the version you like to test: $ tree my-operator/ my-operator/ \u251c\u2500\u2500 0.1.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.1.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 0.5.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v0.5.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u251c\u2500\u2500 1.0.0 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2502 \u2514\u2500\u2500 my-operator.v1.0.0.clusterserviceversion.yaml \u2502 \u251c\u2500\u2500 metadata \u2502 \u2502 \u2514\u2500\u2500 annotations.yaml \u2502 \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 2.0.0 \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 my-operator-crd1.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd2.crd.yaml \u2502 \u251c\u2500\u2500 my-operator-crd3.crd.yaml \u2502 \u2514\u2500\u2500 my-operator.v2.0.0.clusterserviceversion.yaml \u251c\u2500\u2500 metadata \u2502 \u2514\u2500\u2500 annotations.yaml \u2514\u2500\u2500 Dockerfile ... Simply build your bundle using the Dockerfile that is part of every Operator bundle. If you are new to this format please consult the operator-registry documentation. If you used operator-sdk to develop your Operator you can also leverage its packaging tooling to create a bundle . To build your bundle simply build the image and push it to a registry of your choice: podman build -f 2.0.0/Dockerfile -t my-operator:v2.0.0 2.0.0/ podman push my-operator:v2.0.0 quay.io/myaccount/my-operator:v2.0.0 With the bundle published to a registry you can now leverage opm to add it to the existing catalog of community operators: for OpenShift this is quay.io/openshift-community-operators/catalog:latest for OperatorHub.io this is quay.io/operatorhubio/catalog:latest opm will create a catalog image with your Operator added, like so: opm index add --bundles quay.io/myaccount/my-operator:v2.0.0 --from-index quay.io/operatorhubio/catalog:latest --tag quay.io/myaccount/my-test-catalog:latest You then push the resulting catalog image to a registry of your choice as well: podman push quay.io/myaccount/my-test-catalog:latest opm also supports multiple container tools via the -c switch. You can also omit the final step to build the catalog image and instead output the Dockerfile that would be used. Consult the help output for that via opm index add --help You now have a catalog image available for OLM to install your Operator version from. Testing Operator Deployment on Kubernetes 1. Installing Operator Lifecycle Manager If you are not using OpenShift you first need to install the Operator Lifecycle Manager on your cluster. The following steps assume you already have a running Kubernetes cluster that is currently selected as your current-context via kubectl . If you, you can quickly spin up a cluster using tools like KIND or minikube mentioned in the pre-requisites section , e.g. kind create cluster Install the Operator Lifecycle Manager using operator-sdk : operator-sdk olm install Verify that OLM installed correctly: operator-sdk olm status This should output something like the following: INFO[0000] Fetching CRDs for version \"0.16.1\" INFO[0000] Fetching resources for version \"0.16.1\" INFO[0002] Successfully got OLM status for version \"0.16.1\" NAME NAMESPACE KIND STATUS operators.operators.coreos.com CustomResourceDefinition Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed olm-operator olm Deployment Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed olm-operator-binding-olm ClusterRoleBinding Installed operatorhubio-catalog olm CatalogSource Installed olm-operators olm OperatorGroup Installed aggregate-olm-view ClusterRole Installed catalog-operator olm Deployment Installed aggregate-olm-edit ClusterRole Installed olm Namespace Installed global-operators operators OperatorGroup Installed operators Namespace Installed packageserver olm ClusterServiceVersion Installed olm-operator-serviceaccount olm ServiceAccount Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed Troubleshooting If any problems are encountered at this step, verify that you have enough permissions to install OLM (you need to be cluster-admin to register its CRDs) and create an issue in the OLM tracker . 2. Adding the catalog containing your Operator Create a CatalogSource instance in the olm namespace to reference in the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : olm spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest Deploy the CatalogSource resource: kubectl apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries: kubectl delete catalogsource operatorhubio-catalog -n olm Verify your custom catalog got loaded: $ kubectl get catalogsource -n olm NAME DISPLAY TYPE PUBLISHER AGE my-test-catalog grpc 3m32s [...] Verify the health of your catalog: kubectl get catalogsource my-test-catalog -n olm -o yaml Troubleshooting The status section of that object have the lastObservedState condition set to READY . If that is not the case (for example if the condition is set to CONNECTING ) check the healthiness of the pod associated to the catalog in the same namespace. kubectl get pod -n olm The name of the pod will carry the name of the CatalogSource object plus 5 random characters. Usually the source of unhealthy catalogs is catalog pods with pull errors due to missing authentication against the registry or non-existent tags. If the pod is actually running check its logs: kubectl logs my-test-catalog-zcq7h -n olm If there are errors in this log please raise them in the operator-registry issue tracker as any problems caused by malformed bundle/packagemanifest metadata should have been caught during catalog creation. 3. View Available Operators Inspect the list of loaded packagemanifests on the system with the following command to filter for your Operator kubectl get packagemanifests | grep my-operator The example should look like this: grep my-operator 1h2m If your Operator appears in this list, the catalog was successfully parsed and the Operator is now available to install. Troubleshooting If it does not appear in this list return to the previous step and check the logs of the catalog pod. If this does not reveal any error messages check the log of both the packageserver pods of OLM in the olm namespace, e.g.: kubectl get logs packageserver-78c99949df-lf26p -n olm In some occassions the Operator definition is in the catalog but cannot be understood due to some malformed package/bundle content. This case the packageserver should present a related error message. If there are errors in this log please raise them in the operator-registry issue tracker as any problems caused by malformed bundle/packagemanifest metadata should have been caught during catalog creation. 4. Create an OperatorGroup An OperatorGroup is used to denote which namespaces your Operator should be watching. It must exist in the namespace where your operator should be deployed, we'll use default in this example. Its configuration depends on whether your Operator supports watching its own namespace, a single other namespace or all namespaces (as indicated by spec.installModes in the CSV). Create the following file as operator-group.yaml if your Operator supports watching its own or a single namespace. If your Operator supports watching all namespaces you can skip this step and proceed to creating the Subscription object in the operators namespace. An OperatorGroup already exists there with spec.targetNamespace empty. This kind of OperatorGroup instructs the Operator to watch all namespaces. apiVersion : operators.coreos.com/v1alpha2 kind : OperatorGroup metadata : name : my-operatorgroup namespace : default spec : targetNamespaces : - default Deploy the OperatorGroup resource: kubectl apply -f operator-group.yaml 5. Create a Subscription The last steps is to ask OLM to install your Operator. A Subscription is created to represent the intent to install an Operator and keep it updated (automatically even) with newer version from the catalog. This requires you to tell OLM which Operator, in which version from which channel you want to install and where the catalog is, that contains the Operator. Save the following to a file named: operator-subscription.yaml : apiVersion : operators.coreos.com/v1alpha1 kind : Subscription metadata : name : my-operator-subscription namespace : default spec : channel : <channel-name> name : my-operator startingCSV : my-operator.v2.0.0 source : my-test-catalog sourceNamespace : olm If your Operator supports watching all namespaces, change the namespace of the namespace of the Subscription from default to operators . (This namespace already has an OperatorGroup ). In any case replace <channel-name> with the contents of channel.name in your package.yaml file if you have the packagemanifest format or with the contents from operators.operatorframework.io.bundle.channels.v1 in annotations.yaml if you have the bundle format. Then create the Subscription : kubectl apply -f operator-subscription.yaml Troubleshooting Note that the Subscription object is not representing your installed Operator (this is the object in the next step). It only intructs OLM to install your Operator and keep it updated (automatically). Any error message in the Subscription status refers to the attempt to install the Operator from the catalog, which is usually caused by incorrect references to the catalog (types in the name of the Operator, catalog, namespace, etc). 6. Verify Operator health Watch your Operator being deployed by OLM from the catalog with the following command. Change the namespace from default to operators if you installed your Subscrption there. Use the watch switch -w : $ kubectl get clusterserviceversion -n default -w NAME DISPLAY VERSION REPLACES PHASE my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Pending my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 InstallReady my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Installing my-operator.v2.0.0 My Operator 2.0.0 my-operator.v1.0.0 Succeeded The ClusterServiceVersion object represents your installed Operator per version. It can take a couple of seconds to be created as part of the Subscription request. There will always be a ClusterServiceVersion in the namespace of the Subscription . If the OperatorGroup was configured to \"watch\" a list or all namespaces, the ClusterServiceVersion object will be copied to all those namespaces. This might take additional time (usually around 30 secs). If the above command succeeds and the ClusterServiceVersion has transitioned to the Succeeded phase you will also find your Operator's Deployment(s) in the same namespace where the Subscription is. This is your Operator running: kubectl get deployment -n default Troubleshooting If the ClusterServiceVersion is in a pending or failed state, problems occurred when trying to install the Operator. There are two common sources: the components that make up the Operator and the Operator binary itself. Problems with the Operator process itself will result in a Deployment that is unhealthy, either due to a crashing Operator pod or other image level problems. In this case debug the Deployment and Operator logs for any error message. Usually there are either bugs in the Operator or insufficient permissions in the RBAC part of the bundle/package metadata which may crash poorly written Operators. Problems with installing the Operator components can be debugged with the InstallPlan object. It contains a blueprint of the Operator and lists all the Kubernetes resources that are required for the Operator to function. It is automatically created by OLM and linked to the Subscription . If the Subscription created an InstallPlan you can refer to via the status block of the Subscription : .status.InstallPlan contains the name of the InstallPlan object which is always in the same namespace as the Subscription . kubectl describe subscription my-operator-subscription -n default ... Status: ... Installplan: API Version: operators.coreos.com/v1alpha1 Kind: InstallPlan Name: install-2c8lf Uuid: 656e2e6b-582a-46e1-867d-4f7e95c24136 Last Updated: 2020-11-01T19:38:01Z State: AtLatestKnown Events: <none> kubectl describe installplan install-2c8lf -n default This will likely be a lengthy output due to the content of every component of the Operator returned. However, in the .status block of this object the name and health of every component is displayed. Problems with InstallPlan components usually stem from malformed components, insufficient permissions, collisions with existing objects etc. It usually needs to be corrected at the Operator metadata level. Testing Operator Deployment on OpenShift The process to test on OpenShift Container Platform and OKD 4.3 or newer is exactly the same as described above with the exception that OLM is already installed. You can use the same CLI steps to test your Operator but it can however also be done via the UI. 1. Create the CatalogSource Create a CatalogSource instance in the openshift-marketplace namespace to reference the Operator catalog image that contains your Operator version to test: apiVersion : operators.coreos.com/v1alpha1 kind : CatalogSource metadata : name : my-test-catalog namespace : openshift-marketplace spec : sourceType : grpc image : quay.io/myaccount/my-test-catalog:latest You can use the OpenShift Console YAML editor for that or deploy the CatalogSource resource on the CLI: oc apply -f catalog-source.yaml If you created your test catalog containing all existing community operators , you should delete the default catalog that OLM ships with to avoid a lot of duplicate entries. To do this navigate to Administration on the main navigation bar on the left and select the Cluster Settings item. In this view, select the Global Configuration tab and filter for OperatorHub : Click on the OperatorHub item and select the YAML tab. In the YAML editor set the disabled property for the community-catalog to true : Click Save . 2. Find your Operator in the OperatorHub UI Go to your OpenShift UI and find your Operator by filtering for the Custom category: 3. Install your Operator from OperatorHub To install your Operator simply click its icon and in the proceeding dialog click Install . You will be asked where to install your Operator. Select either of the desired installation modes, if your Operator supports it and then click Subscribe You will be forwarded to the Subscription Management section of the OLM UI and after a couple of moments your Operator will be transitioning to Installed . 4. Verify Operator health Change to the Installed Operators section in the left-hand navigation menu to verify your Operator's installation status: It should have transitioned into the state InstallationSucceeded . You can now test it by starting to use its APIs. Testing with scorecard If your Operator is up and running you can verify it is working as intended using its APIs. Additionally you can run operator-sdk 's scorecard utility for validating against good practice and correctness of your Operator. Assuming you are still in your top-level directory where my-operator/ is your bundle location following these instructions to test your Operator using the Operator-SDK : Running your Operator with scorecard Additional Resources Cluster Service Version Spec Example Bundle","title":"Explained"},{"location":"testing-operators/#testing-your-operator-with-operator-framework","text":"","title":"Testing your Operator with Operator Framework"},{"location":"testing-operators/#overview","text":"These instructions walk you through how to manually test that your Operator deploys correctly with Operator Framework, when packaged for the Operator Lifecycle Manager. Although your submission will always be tested as part of the CI you can accelerate the process by testing locally. The tests described in this document can also be executed automatically in a single step using a test suite A previous version of this document required quay.io, operator-courier and operator-marketplace to conduct the tests. This is no longer required.","title":"Overview"},{"location":"testing-operators/#accepted-contribution-formats","text":"The process below assumes that you have a Kubernetes Operator in either of the two following formats supported by the Operator Framework:","title":"Accepted Contribution formats"},{"location":"testing-operators/#pre-requisites","text":"","title":"Pre-Requisites"},{"location":"testing-operators/#operator-metadata-validation","text":"If you are using packagemanifest format you will need to convert your metadata to bundle format for the validation step. In theory the previous operator-courier tool still works but it is no longer maintained.","title":"Operator Metadata Validation"},{"location":"testing-operators/#publishing-your-operator-metadata-to-a-catalog-for-testing","text":"","title":"Publishing your Operator metadata to a catalog for testing"},{"location":"testing-operators/#testing-operator-deployment-on-kubernetes","text":"","title":"Testing Operator Deployment on Kubernetes"},{"location":"testing-operators/#testing-operator-deployment-on-openshift","text":"The process to test on OpenShift Container Platform and OKD 4.3 or newer is exactly the same as described above with the exception that OLM is already installed. You can use the same CLI steps to test your Operator but it can however also be done via the UI.","title":"Testing Operator Deployment on OpenShift"},{"location":"testing-operators/#testing-with-scorecard","text":"If your Operator is up and running you can verify it is working as intended using its APIs. Additionally you can run operator-sdk 's scorecard utility for validating against good practice and correctness of your Operator. Assuming you are still in your top-level directory where my-operator/ is your bundle location following these instructions to test your Operator using the Operator-SDK : Running your Operator with scorecard","title":"Testing with scorecard"},{"location":"testing-operators/#additional-resources","text":"Cluster Service Version Spec Example Bundle","title":"Additional Resources"},{"location":"tests-in-pr/","text":"PR Continuous Integration Operators submitted to this repo are automatically tested on a Kubernetes cluster before being merged. The Kubernetes distribution used for testing depends on which directory the operator is submitted to. Ideally all tests should pass before merging. You can test operators locally using following documentation. CI test scripts Test scripts are written in Ansible and located in our upstream-community branch . There are 3 test types. List of tests are shown in the following table. Test type Description kiwi Full operator test lemon Full test of operator to be deployed from scratch orange Full test of operator to be deployed with existing bundles in quay registry all kiwi,lemon,orange [kiwi] - Full operator test Full operator tests - Building bundle image - from packagemanifest format - from bundle format - Sanity check of operator version (when multiple only last test is done) - Validation using operator-sdk validate - Building temporary catalog with one operator version in it - Deployment of operator on kind (k8s) cluster (only for kuberbetes-operator) [lemon] - Test of operator to be deployed from scratch Test if deploy is possible from the scratch. I means creating bundle images and index image. Build all bundle images Build catalog [orange] - Test of operator to be deployed with existing bundles in quay registry Test if operator can be added to index from existing bundles from production (quay.io) Build current operator version locally Use older versions from from quay.io Build catalog OLM Deployment with the OLM involves creating several required manifest files to create CustomResourceDefinitions (CRD's) and the operators' Deployment using its ClusterServiceVersion (CSV) in-cluster. test-operator will create a operator-registry Docker image containing the operators' bundled manifests, and CatalogSource and Subscription manifests that allow the OLM to find the registry image and deploy a particular CSV from the registry, respectively. Failure to successfully deploy an operator using the OLM results in test failure, as all operators are expected to be deployable in this manner. Scorecard The Operator SDK scorecard suggests modifications applicable to an operator based on development best-practices. The scorecard runs static checks on operator manifests and runtime tests to ensure an operator is using cluster resources correctly. A Custom Resource (CR) is created by the scorecard for use in runtime tests, so alm-examples must be populated. The scorecard utility runs through multiple test scenarios, some of which are required and others are optional. Currently the tests are configured like this. Mandatory tests that need to pass for the PR to be accepted: checkspectest - verifies that the CRs have a spec section writingintocrshaseffecttest - verifies that writing into the CR causes the Operator to issue requests against the Kubernetes API server Recommended tests that should pass in order to have a well-behaved operator: checkstatustest - verifies whether the CRs status block gets updated by the Operator to indicate reconciliation. See the scorecard test documentation for more information. test-operator injects a scorecard proxy container and volume into an operators' CSV manifest before deployment; this is necessary to get API server logs, from which the scorecard determines runtime test results. These modifications are not persistent, as they're only needed for testing. Note : no explicit number of points or percentage is necessary to achieve before merging yet . These are suggestions to improve your operator. operator-courier The operator-courier verify command verifies that a set of files is valid and can be bundled and pushed to quay.io . Read the docs for more information. Upstream operators Operators submitted to the upstream-community-operators/ directory are tested against a KIND instance deployed on a [Travis CI][travis-ci] environment. The OLM is installed locally in this case. OpenShift operators Operators submitted to the community-operators/ directory are tested against an OpenShift 4.0 cluster deployed on AWS using the ci-operator .","title":"Tests in PR explained"},{"location":"tests-in-pr/#pr-continuous-integration","text":"Operators submitted to this repo are automatically tested on a Kubernetes cluster before being merged. The Kubernetes distribution used for testing depends on which directory the operator is submitted to. Ideally all tests should pass before merging. You can test operators locally using following documentation.","title":"PR Continuous Integration"},{"location":"troubleshooting/","text":"Troubleshooting PR-traffic-light failures: Operator changes detected with ci changes and 'allow/ci-changes' is not set Do not modify files outside of community-operators and upstream-community-operators, please rebase or fix and push changes. Changes in both 'community-operators' and 'upstream-community-operators' dirs Every operator must have separate PR. Separate for Kubernetes (upstream) and separate for Openshift (community). It helps more precise testing and also granular revert if needed. Sometimes just rebase is needed. Multiple operators are changed Multiple operators in the same stream are updated. This is not allowed. It could be a result of not rebased PR. Please rebase or create separate PR for every operator. We support only a single file modification in case of 'ci.yaml' file. If you want to update it, please make an extra PR with 'ci.yaml' file modification only!!! Please make an extra PR and modify only ci.yaml . Package metadata test (kiwi) test failures: Operator deployment with OLM failed Could be multiple reasons, but first of all, please check if your operator image can be pulled from a public location. This is the most common root cause for operators failing to start. Tests for this situation are planned in backlog, so pipeline will tell you this in near future automatically. csv.Spec.Icon not specified Icon is mandatory, more information here . ci/prow/deploy-operator-on-openshift failures: Test Failed? To investigate every failed Openshift test, please open test Details and click Show all hidden lines . Then scroll to the bottom and check all logs from bottom to top. Logs are ordered according to debugging value, the most important logs are at the bottom, least important and some initialization staff is at the top of every run. Temp index not found. Are your commits squashed? Error message combined with Missing '$OP_NAME' or similar. Majority of tests ending without a temporary index are caused by PR containing too many commits. Please (rebase) squash and force-push. If it is not your case, you can inspect logs from building temporary indexes for test purposes here https://github.com/operator-framework/community-operators/actions?query=workflow%3Aprepare-test-index. Operation cannot be fulfilled on ... the object has been modified; please apply your changes to the latest version and try again This is not an issue, you can ignore it. ImagePullBackOff error message Unable to pull your operator image. Operator deploy tests (lemon/oragne) test failures: All operator versions are already in catalog You are trying to edit an existing operator version. It is not recommended. But there are some exceptions, where you just edit some description or link. In this case, repository maintainers can set appropriate labels to override such errors and approve release pipeline action to overwrite an existing operator.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#pr-traffic-light-failures","text":"Operator changes detected with ci changes and 'allow/ci-changes' is not set Do not modify files outside of community-operators and upstream-community-operators, please rebase or fix and push changes. Changes in both 'community-operators' and 'upstream-community-operators' dirs Every operator must have separate PR. Separate for Kubernetes (upstream) and separate for Openshift (community). It helps more precise testing and also granular revert if needed. Sometimes just rebase is needed. Multiple operators are changed Multiple operators in the same stream are updated. This is not allowed. It could be a result of not rebased PR. Please rebase or create separate PR for every operator. We support only a single file modification in case of 'ci.yaml' file. If you want to update it, please make an extra PR with 'ci.yaml' file modification only!!! Please make an extra PR and modify only ci.yaml .","title":"PR-traffic-light failures:"},{"location":"troubleshooting/#package-metadata-test-kiwi-test-failures","text":"Operator deployment with OLM failed Could be multiple reasons, but first of all, please check if your operator image can be pulled from a public location. This is the most common root cause for operators failing to start. Tests for this situation are planned in backlog, so pipeline will tell you this in near future automatically. csv.Spec.Icon not specified Icon is mandatory, more information here .","title":"Package metadata test (kiwi) test failures:"},{"location":"troubleshooting/#ciprowdeploy-operator-on-openshift-failures","text":"Test Failed? To investigate every failed Openshift test, please open test Details and click Show all hidden lines . Then scroll to the bottom and check all logs from bottom to top. Logs are ordered according to debugging value, the most important logs are at the bottom, least important and some initialization staff is at the top of every run. Temp index not found. Are your commits squashed? Error message combined with Missing '$OP_NAME' or similar. Majority of tests ending without a temporary index are caused by PR containing too many commits. Please (rebase) squash and force-push. If it is not your case, you can inspect logs from building temporary indexes for test purposes here https://github.com/operator-framework/community-operators/actions?query=workflow%3Aprepare-test-index. Operation cannot be fulfilled on ... the object has been modified; please apply your changes to the latest version and try again This is not an issue, you can ignore it. ImagePullBackOff error message Unable to pull your operator image.","title":"ci/prow/deploy-operator-on-openshift failures:"},{"location":"troubleshooting/#operator-deploy-tests-lemonoragne-test-failures","text":"All operator versions are already in catalog You are trying to edit an existing operator version. It is not recommended. But there are some exceptions, where you just edit some description or link. In this case, repository maintainers can set appropriate labels to override such errors and approve release pipeline action to overwrite an existing operator.","title":"Operator deploy tests (lemon/oragne) test failures:"},{"location":"legacy/using-obsolete-scripts/","text":"Automate testing your Operator locally For convenience, in addition to the manual test instructions we provide a Makefile based test automation. This will automate all the manual steps referred to in Testing Operator Deployment on Kubernetes . In addition the scorecard test from the Operator-SDK will be executed. This is currently tested on Kubernetes in Docker but should work on other Kubernetes systems as well. Prerequisites You need the following installed on your local machine: Linux or macOS host Docker make KIND (if no existing Kubernetes cluster is available via KUBECONFIG or in ~/.kube/config ) Important: Notice, that this script uses a container to execute the test. Your KUBECONFIG will be bind mounted into the container. Therefore no config-helpers or references to files on your host machine are allowed. This is usually the case for minikube or GKE clusters. All further dependencies are encapsulated in a container image that this Makefile will execute as a test driver. Available test modes The Makefile supports two test modes. Both have these supported options: Options: OP_PATH - relative path to your operator (required) OP_VER - version of operator (if not provided the latest will be determined from your package.yaml ) OP_CHANNEL - channel of operator if is not provided it will be parsed by operator package yaml or use the default ones VERBOSE - enable verbose output of executed subcommands Linting metadata only Using operator-courier , this test verifies your CSV and the package definition. More details can be found in the docs . As part of this test nothing will be changed on your system. Example, run from the top-level directory of this repository: make operator . verify OP_PATH = upstream - community - operators / cockroachdb VERBOSE = 1 Pulling docker image [ Processing ] Using default tag : latest latest : Pulling from dmesser / operator - testing Digest : sha256 : 457953575 cd7bd2af60e55fb95f0413195e526c3bbe74b6de30faaf2f10a0585 Status : Image is up to date for quay . io / dmesser / operator - testing : latest Pulling docker image [ OK ] Lint Operator metadata [ Processing ] WARNING : csv metadata . annotations . certified not defined . [ 2 . 0 . 9 / cockroachdb . v2 . 0 . 9 . clusterserviceversion . yaml ] WARNING : csv metadata . annotations . certified not defined . [ 2 . 1 . 1 / cockroachdb . v2 . 1 . 1 . clusterserviceversion . yaml ] Lint Operator metadata [ OK ] Deploying and Testing your Operator Using the Operator Lifecycle Manager (OLM) your Operator will be packaged into a temporary catalog, containing all currently published community operators and yours. OLM will be installed for you if not present. Using the current community catalog as a base allows you to test with dependencies on Operators currently published in this catalog. If you have dependencies outside of this catalog, you need to prepare your own cluster, install OLM, and ship a catalog with these dependencies present; otherwise installation will fail. You can provide a Kubernetes cluster as a testbed via KUBECONFIG or ~/.kube/confg . If you have multiple cluster contexts configured in your KUBECONFIG you will be able to select one. If you have no cluster configured or reachable the Makefile will install a kind cluster named operator-test for you. For this type of test, additionally the following options exist: NO_KIND - if set to 1 no attempt to bring up a kind cluster will be made. In this case you need to specify CATALOG_IMAGE CATALOG_IMAGE - when NO_KIND is set to 1 you need to specify a container registry image location you have push privileges for and from which the image can be pulled again later by OLM without authentication. This parameter is ignored when NO_KIND is absent or set to 0 since the catalog image can be loaded directly into a KIND cluster. CLEAN_MODE - any of NORMAL , NONE and FORCE . As the test installs OLM components in your Kubernetes cluster this controls the clean up of those. In NORMAL clean up will happen if no errors occured. When set to NONE clean up is omitted. When set to FORCE clean up will always be done. Default is NORMAL . INSTALL_MODE - any of OwnNamespace , SingleNamespace , AllNamespaces . this controls the installation mode of the Operator and should be set according to what your Operator states as supported in the installModes section of the CSV. Default is SingleNamepsace . You can start by just deploying your Operator: make operator . install OP_PATH = upstream - community - operators / cockroachdb Pulling docker image [ Processing ] Pulling docker image [ OK ] Find kube config [ / home / dmesser / . kube / config ] Find kube cluster [ Not found ] Start KIND [ Processing ] Start KIND [ OK ] Building catalog image [ Processing ] Building catalog image [ OK ] Operator version detected [ 1.7.2 ] Creating namespace [ Processing ] Creating namespace [ OK ] Verify operator [ Processing ] Verify operator [ OK ] Install OLM [ Processing ] Install OLM [ OK ] Building manifests [ Processing ] Building manifests [ OK ] Operator Deployment [ Processing ] Applying object to cluster [ Processing ] Applying object to cluster [ OK ] Checking if subscriptions passes [ Processing ] Checking if subscriptions passes [ OK ] Checking if CSV passes [ Processing ] Checking if CSV passes [ OK ] Operator Deployment [ OK ] This way you can test if your Operator is packaged correctly. You can also run a test that will deploy your Operator and checks if it behaves correctly according to scorecard (which is part of the Operator-SDK). scorecard will use the example CRs defined in metadata.annotations.alm-examples in the CSV to try to use your Operator and observe its behavior. Example, run from the top-level directory of this repository: [...] make operator.test OP_PATH = upstream-community-operators/cockroachdb [...] Instrumenting Operator for test [ Processing ] creating CR files [ Processing ] creating CR files [ OK ] injecting scorecard proxy [ Processing ] injecting scorecard proxy [ OK ] Instrumenting Operator for test [ OK ] Running scorecard trough all supplied CRs [ Processing ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running required tests [ Processing ] Running required tests [ OK ] Running recommended tests [ Processing ] Running recommended tests [ OK ] Running scorecard trough all supplied CRs [ OK ] Cleaning up Operator resources [ Processing ] Cleaning up Operator resources [ OK ] Cleaning up Operator definition [ Processing ] Cleaning up Operator definition [ OK ] Cleaning up namespace [ Processing ] Cleaning up namespace [ OK ] Troubleshooting Here are some common scenarios, why your test can fail: Failures when linting Operator metadata ERROR: metadata.annotations.alm-examples contains invalid json string [1.4.4/my-operator.v1.4.4.clusterserviceversion.yaml] The linter checks for valid JSON in metadata.annotations.alm-examples . The rest of the CSV is supposed to be YAML. Failures when loading the Operator into the Community Catalog my-operator.v2.1.11 specifies replacement that couldn't be found Explanation: This happens because the catalog cannot load your Operator since it's pointing to a non-existent previous version of your Operator using spec.replaces . For updates, it is important that this property points to another, older version of your Operator that is already in the catalog. error adding operator bundle : error decoding CRD: no kind \\\"CustomResourceDefinition\\\" is registered for version \\\"apiextensions.k8s.io/v1\\\" in scheme \\\"pkg/registry/bundle.go Explanation: Currently OLM does not yet support handling CRDs using apiextensions.k8s.io/v1 . This will improve soon. Until then you need to resort back to apiextensions.k8s.io/v1beta . error loading manifests from directory: error checking provided apis in bundle : couldn't find charts.someapi.k8s.io/v1alpha1/myapi (my-custom-resource) in bundle. found: map[] Explanation: Your Operator claims ownership of a CRD that it does not ship. Check for spelling of Group/Version/Kind in spec.customresourcedefinitions.owned in the CSV. error loading package into db: [FOREIGN KEY constraint failed, no default channel specified for my-operator] Explanation: This happens when either - Your Operator package defines more than one channel in package.yaml but does not define defaultChannel . - The package just defines a single channel (in which case you can omit defaultChannel ) but the catalog couldn't load the CSV that this channel points to using currentCSV . This can happen when in the CSV the specified name in metadata.name is actually different from what currentCSV points to. Failures when deploying via OLM Check if subscription passes times out Explanation: In this case the Subscription object created by the test suite did not transition to the state AtLatestKnown before hitting a timeout. There are various reasons for this, ranging from the catalog pod crashing to problems with the catalog-operator pod of OLM itself. In any case, the logs of either pod will likely help troubleshooting and finding the root cause. Check if CSV passes times out Explanation: OLM could not install the Operator's Deployment from its CSV before hitting a timeout. This is usually due to Deployment reaching its expected replica count, likely because the pod is crash-looping. Failures during tests of the Operator with Operator-SDK scorecard failed to get proxyPod: timed out waiting for the condition: Explanation: If this happened it is likely the Operator pod crashed in the middle of the scorecard test suite. For example, when it failed to parse a Custom Resource fed to scorecard from the list in metadata.annotations.alm-examples . OLM will wait for the Deployment of the Operator to recover before re-installing the Operator. Re-installation changes the Operator pod's name and hence scorecard fails to reach the logs of scorecard proxy using its old name. failed to create cr resource: object is being deleted: someapi.k8s.io \"myCRD\" already exists: Explanation: This can happen when your Operator automatically creates a CR on startup, with the same name of an example for that CR provided in the CSV metadata.annotations.alm-examples section. Simply use a different name in the example. Otherwise, your Operator could be slow to delete a CR due to a finalizer. Additional shortcuts Clean up after a failed test As explained above, the default CLEAN_MODE of NORMAL will delete anything that got installed on your cluster if all tests run correctly.. If something fails, the deployed resources will not be deleted in order to give you a chance to debug. After you have finished debugging you can use the following command to clean up any residual resources as part of a test of a particular Operator: make operator.cleanup OP_PATH=upstream-community-operators/cockroachdb Install a KIND cluster Install a kind cluster as a testbed for the Operator deployment. $ kind create cluster --name operator-test This command will create a Kubernetes in Docker cluster: $ kind get clusters operator-test $ kind get nodes --name operator-test operator-test-control-plane Install Operator Lifecycle Manager Install OLM to an existing cluster (determined via KUBECONFIG or ~/.kube/config ). make olm.install","title":"Automate testing your Operator locally"},{"location":"legacy/using-obsolete-scripts/#automate-testing-your-operator-locally","text":"For convenience, in addition to the manual test instructions we provide a Makefile based test automation. This will automate all the manual steps referred to in Testing Operator Deployment on Kubernetes . In addition the scorecard test from the Operator-SDK will be executed. This is currently tested on Kubernetes in Docker but should work on other Kubernetes systems as well.","title":"Automate testing your Operator locally"},{"location":"legacy/using-obsolete-scripts/#prerequisites","text":"You need the following installed on your local machine: Linux or macOS host Docker make KIND (if no existing Kubernetes cluster is available via KUBECONFIG or in ~/.kube/config ) Important: Notice, that this script uses a container to execute the test. Your KUBECONFIG will be bind mounted into the container. Therefore no config-helpers or references to files on your host machine are allowed. This is usually the case for minikube or GKE clusters. All further dependencies are encapsulated in a container image that this Makefile will execute as a test driver.","title":"Prerequisites"},{"location":"legacy/using-obsolete-scripts/#available-test-modes","text":"The Makefile supports two test modes. Both have these supported options:","title":"Available test modes"},{"location":"legacy/using-obsolete-scripts/#troubleshooting","text":"Here are some common scenarios, why your test can fail:","title":"Troubleshooting"},{"location":"legacy/using-obsolete-scripts/#additional-shortcuts","text":"","title":"Additional shortcuts"}]}